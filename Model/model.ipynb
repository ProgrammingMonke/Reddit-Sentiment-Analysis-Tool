{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets\n",
    "!pip install conllu\n",
    "!pip3 install torchvision\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../API')\n",
    "\n",
    "import api\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "db = api.connect_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 0, -1, 1, 0, 1, -1, 0, -1, 0, -1, 0, 1, 0, 0, 0, 1, 0, 1, -1, -1, 0, 0, -1, 1, -1, 0, 1, 0, 1, -1, -1, 1, 1, 1, 0, 1, 0, 0, 1, -1, 1, -1, -1, 1, 0, 0, -1, 0, 0, 1, 0, 1, -1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, -1, -1, 1, 1, -1, 1, -1, 1, 1, 0, 1, 1, 1, 0, 1, -1, -1, 0, 1, 1, 1, -1, 1, 0, 0, -1, -1, -1, 0, 1, -1, 1, 0, 0, -1, 1, 1, 0, 0, -1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, -1, -1, 1, -1, 0, 1, 1, 0, -1, 1, -1, 0, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 0, 0, 0, 0, 0, -1, 1, 1, 0, 1, 1, 1, 0, -1, -1, 0, 0, 1, -1, 0, 0, -1, 0, 1, -1, 1, 1, -1, 0, 1, 1, -1, 0, 1, 0, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, -1, 1, 1, 1, 1, -1, 0, 1, 1, 1, -1, -1, 1, -1, -1, 0, 0, -1, -1, -1, 1, 0, 1, 0, -1, 1, 1, 1, 1, 0, -1, 0, 0, 0, 1, 1, 1, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 1, 1, 0, 1, -1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 1, -1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 1, -1, -1, 0, 1, -1, 0, 1, 0, 0, 0, 0, 1, -1, 1, 0, 0, -1, 0, 1, 1, 1, 1, -1, 0, 0, 0, 0, 1, 0, 0, -1, 1, -1, 1, 0, -1, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 0, 1, -1, 0, -1, -1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 1, 1, -1, 0, -1, 0, -1, 1, 0, 0, 1, -1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, -1, 0, 1, 1, 1, -1, 0, 1, -1, 1, 1, 1, 0, 0, 0, 1, 0, 1, -1, -1, 1, -1, 0]\n"
     ]
    }
   ],
   "source": [
    "training_data, testing_data = api.get_database_content(db, .8, .2, 0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "training_true_labels = []\n",
    "for title in training_data:\n",
    "    training_true_labels.append(title['label'])\n",
    "\n",
    "testing_true_labels = []\n",
    "for title in testing_data:\n",
    "    testing_true_labels.append(title['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier(nn.Module):\n",
    "  def __init__(self, embedder, num_labels=2):\n",
    "    super().__init__()\n",
    "    self.num_labels = num_labels\n",
    "    self.embedder = embedder\n",
    "    self.fc = nn.Linear(embedder.embedding_size, num_labels)\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.fc(self.embedder(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryWordOverlapBatched:\n",
    "  \"\"\"return a binary vector the size of the vocab, indicating whether each\n",
    "  word occurred in both the premise and hypothesis\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embedding_size = self.vocab_size\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    \"\"\"assume inputs is list of instances of the dict returned by tokenize_rte function\"\"\"\n",
    "    OH = lambda x: nn.functional.one_hot(torch.as_tensor(x), self.embedding_size)\n",
    "    padder = lambda x: nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "\n",
    "    tokens_sent1 = padder([OH(inst['title']['input_ids'][0]) for inst in inputs])\n",
    "\n",
    "    overlap = tokens_sent1.sum(axis=1)\n",
    "    return overlap.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': {'input_ids': tensor([[  101,  2181,  1103, 27140,   170,  9640,  2369,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Is the IDF a terrorist organization?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Is', 'the', 'IDF', 'a', 'terrorist', 'organization', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1262,  5871,  7941,  1209,  1129, 11289,  1111,   107,  4440,\n",
      "           107,  1103, 21978,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'And hamas will be blamed for \"breaking\" the truce', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'And', 'ha', '##mas', 'will', 'be', 'blamed', 'for', '\"', 'breaking', '\"', 'the', 'truce', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8776,  8988,  2114,  1107,   160,   119,  2950,  7005, 16621,\n",
      "          1160,  1441,  4806,  1104, 26470,  1111,  3103,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Armed Palestinian groups in W. Bank reportedly execute two men accused of spying for Israel', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Armed', 'Palestinian', 'groups', 'in', 'W', '.', 'Bank', 'reportedly', 'execute', 'two', 'men', 'accused', 'of', 'spying', 'for', 'Israel', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 16245,  1121,  9276,  1274,  3798,  6630,  1111,  8619,   106,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Grandma from Bosnia donating charity for Palestine!', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Grandma', 'from', 'Bosnia', 'don', '##ating', 'charity', 'for', 'Palestine', '!', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  2088,  1260, 11379,  1126,  4475,  8988,  1656,  1126,\n",
      "         13342,  1107,  1103,  1537,  2950,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli forces detain an injured Palestinian inside an ambulance in the West Bank', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'forces', 'de', '##tain', 'an', 'injured', 'Palestinian', 'inside', 'an', 'ambulance', 'in', 'the', 'West', 'Bank', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  4699,  1362, 21744,  1412,  2147,  1110,  1114, 24574,\n",
      "          1136, 23755,  1867,  3103, 14582,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The Arab world understands our fight is with Hamas not Palestinians says Israel analyst', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'The', 'Arab', 'world', 'understands', 'our', 'fight', 'is', 'with', 'Hamas', 'not', 'Palestinians', 'says', 'Israel', 'analyst', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1790,  112,  189, 1831, 6303, 1164, 8619,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Don't stop sharing about Palestine \", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Don', \"'\", 't', 'stop', 'sharing', 'about', 'Palestine', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3517,  1103,  8988, 15919, 12062,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'About the Palestinian Hostages', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'About', 'the', 'Palestinian', 'Host', '##ages', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1110,  7965,  2394,  3516,  8988, 21832,   789,  5419,\n",
      "           790,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why is mainstream media calling Palestinian hostages “prisoners”?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'is', 'mainstream', 'media', 'calling', 'Palestinian', 'hostages', '“', 'prisoners', '”', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 22515,   118,   151, 10486, 26868,  3291,  5430,  7645,  1103,\n",
      "          1107, 15319,  1162,  3252,  1104, 23755,  1107,  1147,  1319,  1657,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Ta-Nehisi Coates describing the inhumane treatment of Palestinians in their own land.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Ta', '-', 'N', '##eh', '##isi', 'Co', '##ates', 'describing', 'the', 'in', '##human', '##e', 'treatment', 'of', 'Palestinians', 'in', 'their', 'own', 'land', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1422, 3089, 1689, 5370, 7382,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'My Middle East Peace Plan', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'My', 'Middle', 'East', 'Peace', 'Plan', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1258, 1184, 1103, 5871, 7941, 1441, 1225,  119,  119,  119,  119,\n",
      "          119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'After what the hamas men did.....', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'After', 'what', 'the', 'ha', '##mas', 'men', 'did', '.', '.', '.', '.', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1130,  7630,   131,  3103,  2972,  5126,  1104,  5915,  1107,\n",
      "         15109,  1170,  3615,  1552,  1104,  8602,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'In photos: Israel leaves trail of destruction in Gaza after 48 days of bombing', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'In', 'photos', ':', 'Israel', 'leaves', 'trail', 'of', 'destruction', 'in', 'Gaza', 'after', '48', 'days', 'of', 'bombing', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1792,  1106,  4878,   157,  4847, 18290,  1200,   131,  1130,\n",
      "          3103,   117,  1122,  1110,  5696,  1106,  6268,  1105,  1437,   170,\n",
      "          3014,  5394,  2774,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'According to Israeli Tiktoker: In Israel, it is illegal to obtain and show a simple DNA test', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'According', 'to', 'Israeli', 'T', '##ik', '##tok', '##er', ':', 'In', 'Israel', ',', 'it', 'is', 'illegal', 'to', 'obtain', 'and', 'show', 'a', 'simple', 'DNA', 'test', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 12956,  3103,   117,  4268,  2653,  1143,  1111,  1139,   789,\n",
      "          5096,  4163,  3820,  1810,   790,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Dear Israel, please pay me for my “Propaganda”', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Dear', 'Israel', ',', 'please', 'pay', 'me', 'for', 'my', '“', 'Pro', '##pa', '##gan', '##da', '”', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 10684,  7501,  2001, 22226,   117, 23725,   117,  1357,  4766,\n",
      "         17881,  1495, 24574, 13704,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Yuval Lansky, Survivor, October 7th 2023 Hamas Attack', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Yu', '##val', 'La', '##nsky', ',', 'Survivor', ',', 'October', '7th', '202', '##3', 'Hamas', 'Attack', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 17067,  1174,  1107,  1293, 23755,  2458, 17706,   787,   188,\n",
      "          2689,  3797,  1106,  3103,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Interested in how Palestinians view Jew’s religious connection to Israel?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Interest', '##ed', 'in', 'how', 'Palestinians', 'view', 'Jew', '’', 's', 'religious', 'connection', 'to', 'Israel', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2091, 1234, 2140, 1341, 4384, 1132, 1136, 6854, 1106, 3103,  136,\n",
      "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Do people actually think Jews are not indigenous to Israel?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Do', 'people', 'actually', 'think', 'Jews', 'are', 'not', 'indigenous', 'to', 'Israel', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1109, 1178, 1236, 1142, 4139, 1156, 1322,  795,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The only way this conflict would end…', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'The', 'only', 'way', 'this', 'conflict', 'would', 'end', '…', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,  1444,  1106,  1525,  1412,  1191,  1800,   146,  1221,\n",
      "          1107,  1103, 25021,  2087,  1110,  1253, 21534,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'I need to find our if someone I know in the idf is still ok', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'I', 'need', 'to', 'find', 'our', 'if', 'someone', 'I', 'know', 'in', 'the', 'id', '##f', 'is', 'still', 'ok', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 14685,  1133,  1173,  1254,  1175,  1132, 22042,   117,  1315,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Indigenous but then again there are converts, too', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Indigenous', 'but', 'then', 'again', 'there', 'are', 'converts', ',', 'too', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  4554,  2050, 17054,  7281,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A palestinian perspective', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'pale', '##st', '##inian', 'perspective', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 10736,  2256,  1950,  4972,  1106,  1631,   170,  4162,  9304,\n",
      "          5773,  1303,  1107,  1738,  1105,  1631,  4264,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Has anyone else begun to feel a storm brew here in America and feel concerned?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Has', 'anyone', 'else', 'begun', 'to', 'feel', 'a', 'storm', 'br', '##ew', 'here', 'in', 'America', 'and', 'feel', 'concerned', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 8619, 1113,  130,  120, 1429,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestine on 9/11', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Palestine', 'on', '9', '/', '11', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15919,  2553,  5016,  1414,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hostage vs War', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Host', '##age', 'vs', 'War', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7270,  1867,  1185,  8988,  4256,  1280,  1106, 24574,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'EU says no Palestinian aid going to Hamas', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'EU', 'says', 'no', 'Palestinian', 'aid', 'going', 'to', 'Hamas', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  3291, 12140,  2227,  1113,  2743,  4537, 11336,  5521,\n",
      "           112,   188,  4505,   176,  5997,  3970,  1107,  1117,  1319,  1236,\n",
      "          1229, 25317,  1123,   119,   119,   119,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}, 'orig_title': \"Israeli Comment on Little Girl Reem's Father grieving in his own way while burying her....\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'Co', '##mme', '##nt', 'on', 'Little', 'Girl', 'Re', '##em', \"'\", 's', 'Father', 'g', '##rie', '##ving', 'in', 'his', 'own', 'way', 'while', 'burying', 'her', '.', '.', '.', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2181,  1142,  2276,  1115, 23755,  1339,  9480,  1176,  1142,\n",
      "          1107,  1124, 12725,  1179,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Is this true that Palestinians face discrimination like this in Hebron?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Is', 'this', 'true', 'that', 'Palestinians', 'face', 'discrimination', 'like', 'this', 'in', 'He', '##bro', '##n', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4766,  1432,  4699, 10627,  1105,  5915,  1104,  2900,  8708,\n",
      "          1107,  2243,  1746,  1105,  1564,   170,  2087, 15353,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '7th century Arab conquest and destruction of native cultures in middle east and north africa', 'label': -1, 'str_tokenized_sentence': ['[CLS]', '7th', 'century', 'Arab', 'conquest', 'and', 'destruction', 'of', 'native', 'cultures', 'in', 'middle', 'east', 'and', 'north', 'a', '##f', '##rica', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1674,  8619, 18641,  1554,  9076,  1105,  4463,  2266,\n",
      "           136,  1327,  1164,  1639,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why does Palestine deserves full democracy and equal rights ? What about others ?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'does', 'Palestine', 'deserves', 'full', 'democracy', 'and', 'equal', 'rights', '?', 'What', 'about', 'others', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2579, 3070, 4905,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}, 'orig_title': 'Northern border crossing', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Northern', 'border', 'crossing', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1409, 1195, 1125,  170, 1141, 1352, 5072,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'If we had a one state solution', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'If', 'we', 'had', 'a', 'one', 'state', 'solution', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   156, 21277, 10886,  1163,  1115,   131,  1247,  1132,   123,\n",
      "          1550, 13525,  1107, 12776,  1161,  1105,  2687, 11315,   117,  1150,\n",
      "          4819,  1366,  2839,  1112,  1202,  1103, 13525,  1104, 24574,   118,\n",
      "         19432,  6258,  1107, 15109,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Smotrich said that: There are 2 million Nazis in Judea and Samaria, who hate us exactly as do the Nazis of Hamas-ISIS in Gaza.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'S', '##mot', '##rich', 'said', 'that', ':', 'There', 'are', '2', 'million', 'Nazis', 'in', 'Jude', '##a', 'and', 'Sam', '##aria', ',', 'who', 'hate', 'us', 'exactly', 'as', 'do', 'the', 'Nazis', 'of', 'Hamas', '-', 'IS', '##IS', 'in', 'Gaza', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8988,  4225, 12556,  3202,  1830,  8158,  1706,  2328, 15474,\n",
      "          1105,  7425,  1118,  3103,  5262,  4791,   119,  1109,  4225,   787,\n",
      "           188, 27768,  1110,   170,  4267,  2050,  8683,  1181,  2838,  1104,\n",
      "          1103,  2927, 25964,  2340,  1104,  1103,  4878,  1352,   117,  1134,\n",
      "          1110,  7492,  1136,  1178, 23755,  2310,   117,  1133,  1145,  1147,\n",
      "          2754,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}, 'orig_title': 'Palestinian poet Mosab Abu Toha detained and beaten by Israel Defense Forces. The poet’s ordeal is a distilled expression of the barbarity of the Israeli state, which is attacking not only Palestinians themselves, but also their culture', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinian', 'poet', 'Mo', '##sa', '##b', 'Abu', 'To', '##ha', 'detained', 'and', 'beaten', 'by', 'Israel', 'Defense', 'Forces', '.', 'The', 'poet', '’', 's', 'ordeal', 'is', 'a', 'di', '##st', '##ille', '##d', 'expression', 'of', 'the', 'bar', '##bari', '##ty', 'of', 'the', 'Israeli', 'state', ',', 'which', 'is', 'attacking', 'not', 'only', 'Palestinians', 'themselves', ',', 'but', 'also', 'their', 'culture', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109, 10672, 21162,  1107,  3562,  2004,   107,  1109,  6939,\n",
      "          2947, 23220,  9574,   107,  1110,  1177,  6276,  1105,  8026,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The Always sunny in Philadelphia episode \"The gang goes jihad\" is so funny and accurate', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'The', 'Always', 'sunny', 'in', 'Philadelphia', 'episode', '\"', 'The', 'gang', 'goes', 'ji', '##had', '\"', 'is', 'so', 'funny', 'and', 'accurate', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2896,  1320, 23938,   111,   139, 21883,  1113,   170,  3416,\n",
      "         10616,  2000,  2520,  1164,  1357,   128,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Elon Mask & Bibi on a shared Twitter space talking about October 7', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'El', '##on', 'Mask', '&', 'B', '##ibi', 'on', 'a', 'shared', 'Twitter', 'space', 'talking', 'about', 'October', '7', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   548, 28467, 28463, 28454, 28462, 28460, 28454, 28457,   535,\n",
      "         28460, 28453, 28454,   544, 28467, 28466, 28445, 28456, 28454, 28457,\n",
      "           197,  8329,  4878,  1130,  2087, 19224,  3633,  1733,   165,  8500,\n",
      "          1116,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'משפיענים אנטי ישראלים | Anti Israeli Influencers\\\\Pages', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'מ', '##ש', '##פ', '##י', '##ע', '##נ', '##י', '##ם', 'א', '##נ', '##ט', '##י', 'י', '##ש', '##ר', '##א', '##ל', '##י', '##ם', '|', 'Anti', 'Israeli', 'In', '##f', '##lue', '##nce', '##rs', '\\\\', 'Page', '##s', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  1110,  1142,  4841,  4359, 17903,  4893,  1104,  1103,\n",
      "           152, 23370,  1204,  5370,  7382,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What is this subreddit opinion of the Olmert Peace Plan?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'is', 'this', 'sub', '##red', '##dit', 'opinion', 'of', 'the', 'O', '##lmer', '##t', 'Peace', 'Plan', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  1291, 10736, 12798,  1174,  1106,  6335,  1106,  1398,\n",
      "          4064, 23726,  1116,  1106,  1321, 11155,  4388,  1106,  1103,  4118,\n",
      "          4936,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The World Has Progressed to Much to Allow Zionists to take Us Back to the Stone Age ', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', 'World', 'Has', 'Progress', '##ed', 'to', 'Much', 'to', 'All', '##ow', 'Zionist', '##s', 'to', 'take', 'Us', 'Back', 'to', 'the', 'Stone', 'Age', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1132,   170,  1974,  1104, 13073,  4374,  8619,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why are a lot of celebrities supporting Palestine?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'are', 'a', 'lot', 'of', 'celebrities', 'supporting', 'Palestine', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1107, 22352,  1114,   179, 17540,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'in solidarity with jews !', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'in', 'solidarity', 'with', 'j', '##ews', '!', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2825,  787,  189, 1129, 1270, 9640, 1165, 1128, 1132, 6611, 1240,\n",
      "         1313, 1105, 2191,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Can’t be called terrorist when you are defending your home and self', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Can', '’', 't', 'be', 'called', 'terrorist', 'when', 'you', 'are', 'defending', 'your', 'home', 'and', 'self', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8988,  7682, 19722,  3394, 12746,  1106,   182,  6334,  1179,\n",
      "          1343, 26899,  1174,  1107, 15109,  1118, 23726,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinian Christians cancel Christmas celebrations to mourn those martyred in Gaza by Zionist.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinian', 'Christians', 'cancel', 'Christmas', 'celebrations', 'to', 'm', '##our', '##n', 'those', 'martyr', '##ed', 'in', 'Gaza', 'by', 'Zionist', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  2088,  1231,  2293, 22834, 11478,  1479,  1201,  1385,\n",
      "          1302, 14467,  1584, 13030, 22678,  1150,  1108,  1308,  1112,  1226,\n",
      "          1104,  4306,   112,   188,  2239,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, 'orig_title': \"Israeli forces rekidnap 16 years old Nofoz Hammad who was released as part of Saturday's deal\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'forces', 're', '##ki', '##dn', '##ap', '16', 'years', 'old', 'No', '##fo', '##z', 'Ham', '##mad', 'who', 'was', 'released', 'as', 'part', 'of', 'Saturday', \"'\", 's', 'deal', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15109,  1179,  4037,  1289, 14551,  1106, 24574,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Gazan citizens hand hostage to Hamas.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Gaza', '##n', 'citizens', 'hand', 'hostage', 'to', 'Hamas', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101,  146,  112,  182, 5250, 8619,  117,  146, 1253, 1169,  112,  189,\n",
      "         1267, 1293,  107, 1121, 1103, 2186, 1106, 1103, 2343,  107, 1169, 1129,\n",
      "         1625, 1133, 4819, 2365,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}, 'orig_title': 'I\\'m pro Palestine, I still can\\'t see how \"from the river to the sea\" can be anything but hateful', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'I', \"'\", 'm', 'pro', 'Palestine', ',', 'I', 'still', 'can', \"'\", 't', 'see', 'how', '\"', 'from', 'the', 'river', 'to', 'the', 'sea', '\"', 'can', 'be', 'anything', 'but', 'hate', '##ful', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  1106,  1836,  1429, 21832,  1113,  6356,   118,  4498,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas to release 11 hostages on Monday - Egypt', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'to', 'release', '11', 'hostages', 'on', 'Monday', '-', 'Egypt', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,   787,   188,  5664,  1107, 15109, 19432, 16789,  1107,\n",
      "          1142,  1432,   119,  1135,  1169,  1185,  2039,  1129,  3402,  1106,\n",
      "          8755,  1107,  1103,  1954,  1425,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': 'What’s happening in Gaza IS unprecedented in this century. It can no longer be compared to wars in the current age.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', '’', 's', 'happening', 'in', 'Gaza', 'IS', 'unprecedented', 'in', 'this', 'century', '.', 'It', 'can', 'no', 'longer', 'be', 'compared', 'to', 'wars', 'in', 'the', 'current', 'age', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2181,  1142,  2276,  1115, 23755,  1107,  1537,  8607,  1132,\n",
      "          5165, 23236,  1256,  1463,  1152,  1132,  1136,  2935,   179, 17540,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Is this true that Palestinians in Westbank are treated harshly even though they are not fighting jews?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Is', 'this', 'true', 'that', 'Palestinians', 'in', 'West', '##bank', 'are', 'treated', 'harshly', 'even', 'though', 'they', 'are', 'not', 'fighting', 'j', '##ews', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  1372,  1115,  6967,  1103, 11104,  8128,  1110,  1521,\n",
      "          1118,  1103,  4070,  7507,  1104,  1103,  2026,  5696,  3433,  1459,\n",
      "          1404,  1107,  3749,  8619,   117,  1103,   147, 28047,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The group that organised the demonstration yesterday is led by the Vice Chair of the largest illegal settlement building body in occupied Palestine, the JNF.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'The', 'group', 'that', 'organised', 'the', 'demonstration', 'yesterday', 'is', 'led', 'by', 'the', 'Vice', 'Chair', 'of', 'the', 'largest', 'illegal', 'settlement', 'building', 'body', 'in', 'occupied', 'Palestine', ',', 'the', 'J', '##NF', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  1594,  1107, 15109,  1144,  1151,  1126,  5827, 11788,\n",
      "          1107,  2466,   177,  1183,  5674,  1665,  4889,  1183,   119,  1135,\n",
      "          1281,   787,   189,  1129,  6278,   783,  7754,  1646,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}, 'orig_title': 'The war in Gaza has been an intense lesson in western hypocrisy. It won’t be forgotten — Guardian US', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', 'war', 'in', 'Gaza', 'has', 'been', 'an', 'intense', 'lesson', 'in', 'western', 'h', '##y', '##po', '##c', '##ris', '##y', '.', 'It', 'won', '’', 't', 'be', 'forgotten', '—', 'Guardian', 'US', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 13063, 10517,  1106,  4330,  1139,  2537,  1104,  1681, 13828,\n",
      "          1107,  1470,   119,  1135,  1238,   787,   189,  1301,  1218,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Decided to wear my Star of David necklace in public. It didn’t go well.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Dec', '##ided', 'to', 'wear', 'my', 'Star', 'of', 'David', 'necklace', 'in', 'public', '.', 'It', 'didn', '’', 't', 'go', 'well', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,   112,   182,  3008,  1114,  3103,  2479,  8619,   117,\n",
      "          1198,  1136,  1223, 24574,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"I'm okay with Israel becoming Palestine, just not under Hamas.\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'I', \"'\", 'm', 'okay', 'with', 'Israel', 'becoming', 'Palestine', ',', 'just', 'not', 'under', 'Hamas', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   159, 12788,  3418,  1104, 15109, 19643,  2502,  1154,  2458,\n",
      "           119,  2677,  1552,  1154,  1103,  1148,  1519,  4455,  1107,  3103,\n",
      "           787,   188,  1160,   118,  2370,   118,  1263, 17347,  1104, 15109,\n",
      "           117,  1273, 11233,  1138,  4972,  1106,  5830,  1103,  2554,  1286,\n",
      "          1481,  1118,  1103, 16170,  3367,  3513,  1104, 15109,   787,   188,\n",
      "          6688,  1416,  1107,  1184,  1110,  1103,  1362,   787,   188,  2026,\n",
      "          3755,  2741,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Vast scale of Gaza genocide comes into view. Three days into the first letup in Israel’s two-month-long bombardment of Gaza, film crews have begun to document the evidence left behind by the deliberate mass murder of Gaza’s civilian population in what is the world’s largest crime scene', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'V', '##ast', 'scale', 'of', 'Gaza', 'genocide', 'comes', 'into', 'view', '.', 'Three', 'days', 'into', 'the', 'first', 'let', '##up', 'in', 'Israel', '’', 's', 'two', '-', 'month', '-', 'long', 'bombardment', 'of', 'Gaza', ',', 'film', 'crews', 'have', 'begun', 'to', 'document', 'the', 'evidence', 'left', 'behind', 'by', 'the', 'deliberate', 'mass', 'murder', 'of', 'Gaza', '’', 's', 'civilian', 'population', 'in', 'what', 'is', 'the', 'world', '’', 's', 'largest', 'crime', 'scene', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  1132,  1240,  9122,  2146,  1137,  1263,  1532,  2394,\n",
      "          3438,  1113,  1103,  1110, 20439,  1233,   118,  4554,  2050,  2042,\n",
      "          4139,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What are your favourite books or long form media content on the israel-palestine conflict', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'are', 'your', 'favourite', 'books', 'or', 'long', 'form', 'media', 'content', 'on', 'the', 'is', '##rae', '##l', '-', 'pale', '##st', '##ine', 'conflict', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1192,  1132,  1136, 11650,  1106, 11516,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'You are not immune to propaganda', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'You', 'are', 'not', 'immune', 'to', 'propaganda', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2009, 1169, 1175, 3927, 4360, 2656, 2231, 1133, 1136, 1256, 1141,\n",
      "         2778, 1352,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why can there 49 Muslim majority states but not even one Jewish state?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'can', 'there', '49', 'Muslim', 'majority', 'states', 'but', 'not', 'even', 'one', 'Jewish', 'state', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,  4373,  6467, 13966,   118,  2055,  2508, 17868,  7719,\n",
      "          1204,   108,  1110, 20439,  1233,   108,  4554,  2050,  2042,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'I Have Had Enough - Richard Medhurst #israel #palestine', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'I', 'Have', 'Had', 'Enough', '-', 'Richard', 'Me', '##dh', '##urs', '##t', '#', 'is', '##rae', '##l', '#', 'pale', '##st', '##ine', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  4789,  1372,  1107, 15109,  1502,   170,  2998,  1121,\n",
      "          1141,  1104,  1103,  1260, 11379,  8870,  1106,  1103,  7705,  1105,\n",
      "          3478,  1104,  2586,   118,   154, 11192,  2312,  4292,  1116,  1150,\n",
      "          4977,  1123,  1219,  1123, 13826,  1669,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The resistance group in Gaza published a letter from one of the detainees to the fighters and leaders of Al-Qassam Brigades who accompanied her during her detention period.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'The', 'resistance', 'group', 'in', 'Gaza', 'published', 'a', 'letter', 'from', 'one', 'of', 'the', 'de', '##tain', '##ees', 'to', 'the', 'fighters', 'and', 'leaders', 'of', 'Al', '-', 'Q', '##ass', '##am', 'Brigade', '##s', 'who', 'accompanied', 'her', 'during', 'her', 'detention', 'period', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5096,   118, 10342,  4638,  2386,   117,  9198, 13335,  3269,\n",
      "           170, 23043, 25976,  1993,  5820,  2110,   117, 26835,  3080, 12311,\n",
      "          9962,  1867,  3103,  1144,  1136,  3088,  1835,  1644,   117,  1144,\n",
      "           107,  1678,  1632, 24782,   107,  1106,   170, 22742,  1118,  1122,\n",
      "          1105,   107,  1195, 12647, 15554,  1181,  1172,  1111,  1115,   107,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}, 'orig_title': 'Pro-Apartheid, Genocide apologist UK Trade Minister, Kemi Badenoch says Israel has not broken international law, has \"taken great pains\" to abide by it and \"we applaud them for that\".', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Pro', '-', 'Apart', '##he', '##id', ',', 'Gen', '##oc', '##ide', 'a', '##pol', '##ogist', 'UK', 'Trade', 'Minister', ',', 'Ke', '##mi', 'Baden', '##och', 'says', 'Israel', 'has', 'not', 'broken', 'international', 'law', ',', 'has', '\"', 'taken', 'great', 'pains', '\"', 'to', 'a', '##bide', 'by', 'it', 'and', '\"', 'we', 'app', '##lau', '##d', 'them', 'for', 'that', '\"', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2892,  1104,  1103,  1805,  1104, 12776,  1161,   118,  2687,\n",
      "         11315,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'History of the region of Judea-Samaria', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'History', 'of', 'the', 'region', 'of', 'Jude', '##a', '-', 'Sam', '##aria', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  6556,  1104,  1103,  1482,  1137, 13663,  1841,  1107,\n",
      "         15109,  1127,  4127,  7418,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What percentage of the children or teenagers killed in Gaza were combatants ?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'What', 'percentage', 'of', 'the', 'children', 'or', 'teenagers', 'killed', 'in', 'Gaza', 'were', 'combat', '##ants', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2372,   181,  1403, 21238,  1234,  2140,  1841,  1107,  1103,\n",
      "          1537,  2950,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Are lgbt people actually killed in the West Bank?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Are', 'l', '##g', '##bt', 'people', 'actually', 'killed', 'in', 'the', 'West', 'Bank', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1960, 16346,  1116,  1104,  1297,  1107,  1103,  3749,  1537,\n",
      "          2950,  1392,  1104,  1124, 12725,  1179,   113,  2586,   148, 24486,\n",
      "          1233,   117,  1106,  1103, 23755,   114,  1292,  1132,  1121,  1263,\n",
      "          1196, 14125,  4766,   119,  1188,  1110,  1103,  1352,  1104,  5707,\n",
      "          1112,  1152,   112,  1396,  1866,  1111,  4397,  1208,   119,  3518,\n",
      "          1112,  4400,  1107,  1103,   195,  1988,  1776,  9127,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Two slices of life in the occupied West Bank City of Hebron (Al Khalil, to the Palestinians) these are from long before Oct 7th. This is the state of affairs as they've stood for decades now. Business as usual in the zionist entity.\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Two', 'slice', '##s', 'of', 'life', 'in', 'the', 'occupied', 'West', 'Bank', 'City', 'of', 'He', '##bro', '##n', '(', 'Al', 'K', '##hali', '##l', ',', 'to', 'the', 'Palestinians', ')', 'these', 'are', 'from', 'long', 'before', 'Oct', '7th', '.', 'This', 'is', 'the', 'state', 'of', 'affairs', 'as', 'they', \"'\", 've', 'stood', 'for', 'decades', 'now', '.', 'Business', 'as', 'usual', 'in', 'the', 'z', '##ion', '##ist', 'entity', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2677,  8988,  1651,  2046,  1107, 16402,   117,  8472,  1107,\n",
      "          6321,  4819,  3755,   131,  4306,   787,   188,  4598,  1110,  1226,\n",
      "          1104,   170,   176,  2254,  6906,  1104,  2848,   118,  4360,  1105,\n",
      "          2848,   118,  4699,  4819,  1105, 15069,  1115,  1144,  1151,  6182,\n",
      "          1118,  6822, 23726,  1116,   117,  7749,  1776,  8673,  1105,  9024,\n",
      "          1451,  3181, 14688,  1796, 12778,  1182,   118,  5534,  2586,   147,\n",
      "         10961, 27472,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Three Palestinian students shot in Burlington, Vermont in suspected hate crime: Saturday’s shooting is part of a geyser of anti-Muslim and anti-Arab hate and bias that has been encouraged by wealthy Zionists, imperialist politicians and virtually every press outlet outside Qatari-backed Al Jazeera.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Three', 'Palestinian', 'students', 'shot', 'in', 'Burlington', ',', 'Vermont', 'in', 'suspected', 'hate', 'crime', ':', 'Saturday', '’', 's', 'shooting', 'is', 'part', 'of', 'a', 'g', '##ey', '##ser', 'of', 'anti', '-', 'Muslim', 'and', 'anti', '-', 'Arab', 'hate', 'and', 'bias', 'that', 'has', 'been', 'encouraged', 'by', 'wealthy', 'Zionist', '##s', ',', 'imperial', '##ist', 'politicians', 'and', 'virtually', 'every', 'press', 'outlet', 'outside', 'Qatar', '##i', '-', 'backed', 'Al', 'J', '##az', '##eera', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2677, 8988, 1651, 2046, 1107, 1646, 1107, 3607, 3879,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Three Palestinian students shot in US in critical condition', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Three', 'Palestinian', 'students', 'shot', 'in', 'US', 'in', 'critical', 'condition', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1130, 11824,  1158,  2035,  1113,  1714,  4055,   117,  2175,\n",
      "          2492, 25905,  1222,  1470, 15024,   787,  3779,  6021, 10137, 15109,\n",
      "         19643,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'In chilling attack on free speech, court issues injunction against public defenders’ union resolution opposing Gaza genocide', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'In', 'chill', '##ing', 'attack', 'on', 'free', 'speech', ',', 'court', 'issues', 'injunction', 'against', 'public', 'defenders', '’', 'union', 'resolution', 'opposing', 'Gaza', 'genocide', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1422,   188, 14185,  3715,  1278,  1144,  1103,  8619,  3441,\n",
      "          4520,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'My slovak school has the Palestine territory map', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'My', 's', '##lov', '##ak', 'school', 'has', 'the', 'Palestine', 'territory', 'map', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2404,  1883,   151, 14272,  1200,  1113,   145,  2069,  2924,\n",
      "           112,   188,  7516,  2530,   192,   120, 12747,  1106,  3103,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Hillel Neuer on HRW's reporting standard w/ regards to Israel\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Hill', '##el', 'N', '##eu', '##er', 'on', 'H', '##R', '##W', \"'\", 's', 'reporting', 'standard', 'w', '/', 'regards', 'to', 'Israel', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   112, 12735, 13107,   112,   131,  3921,  7667, 14177, 17770,\n",
      "          4299,  1181, 15919, 12062,   112, 13359, 11381,  4777,  2614,  1130,\n",
      "         15914,  6366,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"'Complete Darkness': Family Members Describe Freed Hostages' Frightening Time In Captivity\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', \"'\", 'Complete', 'Darkness', \"'\", ':', 'Family', 'Members', 'Des', '##cribe', 'Free', '##d', 'Host', '##ages', \"'\", 'Fr', '##ight', '##ening', 'Time', 'In', 'Capt', '##ivity', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4243,  5399,  8553,  1502,  1126,  3342,   789, 18036,  1116,\n",
      "          1113,  1357,  1542,  2393,   118,  2586,  3031,  3355, 16409,  1643,\n",
      "          8867,  1988,   790,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Human Rights Watch published an article “Findings on October 17 al-Alhi Hospital Explosion”', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Human', 'Rights', 'Watch', 'published', 'an', 'article', '“', 'Finding', '##s', 'on', 'October', '17', 'al', '-', 'Al', '##hi', 'Hospital', 'Ex', '##p', '##los', '##ion', '”', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1646,  2601, 11877,  1130, 22398,  8988,  1426,   786,  2181,\n",
      "          1103,  2809,  4714, 15322,  1111,  3103,   787,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'US Lawmakers Insist Palestinian State ‘Is the Only Way Forward for Israel’', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'US', 'Law', '##makers', 'In', '##sist', 'Palestinian', 'State', '‘', 'Is', 'the', 'Only', 'Way', 'Forward', 'for', 'Israel', '’', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 12118, 18299,  5552,  4893,  1164,  1103,  1836,  1104,  4878,\n",
      "         21832,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Unpopular opinion about the release of Israeli hostages', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Un', '##pop', '##ular', 'opinion', 'about', 'the', 'release', 'of', 'Israeli', 'hostages', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 3103, 4646,  119,  138, 1207, 2174, 1110, 1303,  119, 1327, 1674,\n",
      "         1122, 1440, 1176,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel wins. A new future is here. What does it look like?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'wins', '.', 'A', 'new', 'future', 'is', 'here', '.', 'What', 'does', 'it', 'look', 'like', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 14549,  1121,  1308, 21832,  1106,  1103,  3478,  1105,  7705,\n",
      "          1104,  2586,   154, 11192,  2312,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Letter from released hostages to the leaders and fighters of Al Qassam', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Letter', 'from', 'released', 'hostages', 'to', 'the', 'leaders', 'and', 'fighters', 'of', 'Al', 'Q', '##ass', '##am', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  6315,  3455,  2190,  1164,  8619,   118,  2892,  1105,\n",
      "         11207,   120,  7594,   120, 14536,  5063,  1105, 24318, 16912,  1116,\n",
      "           120, 17601,  1616,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A recommended reading list about Palestine - History and Politics /Literature/ Comic Books and Graphic Novels/ Cookery', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'recommended', 'reading', 'list', 'about', 'Palestine', '-', 'History', 'and', 'Politics', '/', 'Literature', '/', 'Comic', 'Books', 'and', 'Graphic', 'Novel', '##s', '/', 'Cooke', '##ry', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2408,  2848,   118,  4878,  1116,  1132,  6330,  1190,  1110,\n",
      "         20439,  1233,   120,  1103, 27140,  3576, 23661,  6581,   120,   189,\n",
      "         24887,  1116,  3576,  1164,  1614,  1176,  1103, 11175,  1105,  8894,\n",
      "           117,  1105,  1173, 18931,  1103,  8345,  1105,  1231,  4455, 18268,\n",
      "          5045,  6581,   119,  1327,  1202,  1128,  1474,  1106,  1115,   136,\n",
      "          1247,   787,   188,  5544,  3764,  1869,  1303,  4476,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Many anti-Israelis are claiming than israel/the IDF etc uploaded videos/tweets etc about things like the tunnels and hospitals, and then deleted the posts and reuploaded edited videos. What do you say to that? There’s obviously missing information here somewhere', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Many', 'anti', '-', 'Israeli', '##s', 'are', 'claiming', 'than', 'is', '##rae', '##l', '/', 'the', 'IDF', 'etc', 'uploaded', 'videos', '/', 't', '##weet', '##s', 'etc', 'about', 'things', 'like', 'the', 'tunnels', 'and', 'hospitals', ',', 'and', 'then', 'deleted', 'the', 'posts', 'and', 're', '##up', '##loaded', 'edited', 'videos', '.', 'What', 'do', 'you', 'say', 'to', 'that', '?', 'There', '’', 's', 'obviously', 'missing', 'information', 'here', 'somewhere', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1274,   112,   189,  1103,  1234,  1104,  8619,  7338,\n",
      "         24574,  1111,  1136,  7890,  1158,  1172,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Why don't the people of Palestine blame Hamas for not sheltering them?\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'don', \"'\", 't', 'the', 'people', 'of', 'Palestine', 'blame', 'Hamas', 'for', 'not', 'shelter', '##ing', 'them', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 23755,  5419,  1339,   170,  1830, 13252, 17759,  2975,  1107,\n",
      "          4878, 20070,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinians prisoners face abhorrent conditions in Israeli prisons', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinians', 'prisoners', 'face', 'a', '##b', '##hor', '##rent', 'conditions', 'in', 'Israeli', 'prisons', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2091,  1220,  2431,  9224,  1327,   138,  9198, 13335,  3269,\n",
      "          2181,  1335,  1188,  4221,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Do They Even Know What A Genocide Is At This Point', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Do', 'They', 'Even', 'Know', 'What', 'A', 'Gen', '##oc', '##ide', 'Is', 'At', 'This', 'Point', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   789,  1422,  1271,  1110,  2816,   117,  1133, 13280,  1136,\n",
      "          2816,   790,  3802,  1121,   170,  2027,  1690,  1107, 15109,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '“My name is happy, but im not happy” message from a child living in Gaza.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '“', 'My', 'name', 'is', 'happy', ',', 'but', 'im', 'not', 'happy', '”', 'message', 'from', 'a', 'child', 'living', 'in', 'Gaza', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   112, 11569, 11932,   112,  1121,  3470,  1107, 15109,\n",
      "           117,  1155, 27487,  1116,  2266,  1372,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Israel 'stealing organs' from bodies in Gaza, alleges rights group\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', \"'\", 'stealing', 'organs', \"'\", 'from', 'bodies', 'in', 'Gaza', ',', 'all', '##ege', '##s', 'rights', 'group', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2777, 1202, 1195, 1267, 3103, 1105, 8619, 1275, 1201, 1121, 1208,\n",
      "          136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Where do we see Israel and Palestine 10 years from now?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Where', 'do', 'we', 'see', 'Israel', 'and', 'Palestine', '10', 'years', 'from', 'now', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  6983, 20452, 22332,  2339,  1104,  1109, 11300,  2093,  6451,\n",
      "         22955,  4878, 11516,  1213,  2586,   118, 14104,  8057,  2704,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Jeremy Scahill of The Intercept destroys Israeli propaganda around Al-Shifa hospital', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Jeremy', 'Sc', '##ahi', '##ll', 'of', 'The', 'Inter', '##ce', '##pt', 'destroys', 'Israeli', 'propaganda', 'around', 'Al', '-', 'Shi', '##fa', 'hospital', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3743,  1104, 26375,   118, 21105,  1158,  2052,   146,  1189,\n",
      "          1142, 14525,  1111,  8619, 22352,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Instead of doom-scrolling today I made this poster for Palestine solidarity', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Instead', 'of', 'doom', '-', 'scroll', '##ing', 'today', 'I', 'made', 'this', 'poster', 'for', 'Palestine', 'solidarity', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7187,  3103,  2644,  2310,  1118,  1136,  6783, 25568,  1115,\n",
      "          1103, 11896,  1377,  2822,  1108,  2488,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Does Israel hurt themselves by not publicly acknowledging that the Nakba was wrong?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Does', 'Israel', 'hurt', 'themselves', 'by', 'not', 'publicly', 'acknowledging', 'that', 'the', 'Na', '##k', '##ba', 'was', 'wrong', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2009, 1144, 3103, 1151, 1682, 1106, 1294, 3519, 1114, 4421, 1105,\n",
      "         4498,  117, 1133, 1136, 8619,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why has Israel been able to make peace with Jordan and Egypt, but not Palestine?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'has', 'Israel', 'been', 'able', 'to', 'make', 'peace', 'with', 'Jordan', 'and', 'Egypt', ',', 'but', 'not', 'Palestine', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 5096,  118, 8988, 1303,  117, 2367, 1143, 3243,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Pro-Palestinian here, ask me questions', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Pro', '-', 'Palestinian', 'here', ',', 'ask', 'me', 'questions', '[SEP]']}, {'title': {'input_ids': tensor([[ 101,  153, 6447, 1158, 1111, 1302, 1161,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Praying for Noa', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'P', '##ray', '##ing', 'for', 'No', '##a', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  1242,  4384,  1127, 13577,  1656,  2268,  1810,  6207,\n",
      "          8619,  1196,  1105,  1219,  1103,  1414,  1104,  7824,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How many Jews were displaced inside Mandatory Palestine before and during the War of Independence?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'How', 'many', 'Jews', 'were', 'displaced', 'inside', 'Man', '##da', '##tory', 'Palestine', 'before', 'and', 'during', 'the', 'War', 'of', 'Independence', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1731, 1225, 4384, 6268, 1657, 2988, 1106, 3027,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How did Jews obtain land prior to 1948?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'did', 'Jews', 'obtain', 'land', 'prior', 'to', '1948', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 21991,  9068,  1121, 15109, 10100,  1725,  4789,  1538,  1129,\n",
      "          2726,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Important commentary from Gaza explaining why resistance must be supported', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Important', 'commentary', 'from', 'Gaza', 'explaining', 'why', 'resistance', 'must', 'be', 'supported', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  1110, 18147,   170, 19643,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel is committing a genocide', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'is', 'committing', 'a', 'genocide', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 6405, 5419,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}, 'orig_title': 'Child prisoners', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Child', 'prisoners', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1202,  4554,  2050, 17054,  1116,  1341,  1152,  1169,\n",
      "          3513,   117,  8991,  1657,  1105,  1712,  1122,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why do palestinians think they can murder, steal land and keep it?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'do', 'pale', '##st', '##inian', '##s', 'think', 'they', 'can', 'murder', ',', 'steal', 'land', 'and', 'keep', 'it', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  9846,  1105, 25070, 20781,  1113, 15109,  3808,  1112,  4288,\n",
      "          3949, 15835,  1622,  1414,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Tears and Laughter on Gaza Beach as Children Get Break From War', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Tears', 'and', 'Lau', '##ghter', 'on', 'Gaza', 'Beach', 'as', 'Children', 'Get', 'Break', 'From', 'War', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  1203,   138,  8661, 20073,  1111,  1103,  1414,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A New Approach for the War', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'A', 'New', 'A', '##pp', '##roach', 'for', 'the', 'War', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2896,  1320, 19569,  5276,   112,   188,  3103,  3143,  1108,\n",
      "           170, 22572,  4626,  2007,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Elon Musk's Israel visit was a charade\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'El', '##on', 'Mu', '##sk', \"'\", 's', 'Israel', 'visit', 'was', 'a', 'ch', '##ara', '##de', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1203, 16605, 24282,  8145,  1121,  1103,   128,   120,  1275,\n",
      "           131, 24574, 16621,  1116,  1126,  4878,  6688,  1120,  1553,  9153,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'New dashcam footage from the 7/10: Hamas executes an Israeli civilian at point blank.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'New', 'dash', '##cam', 'footage', 'from', 'the', '7', '/', '10', ':', 'Hamas', 'execute', '##s', 'an', 'Israeli', 'civilian', 'at', 'point', 'blank', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1790,   787,   189,  2303,  1111,   171,  8032,  1361,  3711,\n",
      "          1104,   786,  6489,  4184,  5114, 10242,   787,   782,  1109, 27629,\n",
      "          8355,  1104,  6489,  4184,  5114, 10242,  1110,  1215,  1106,  3747,\n",
      "          1251,  5879,  1104,  1103,  4769,  1362,   117,  1259,  4769,  4252,\n",
      "          7877, 20279,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Don’t fall for bogus claims of ‘Islamophobia’ – The taunt of Islamophobia is used to silence any criticism of the Islamic world, including Islamic extremism', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Don', '’', 't', 'fall', 'for', 'b', '##og', '##us', 'claims', 'of', '‘', 'Islam', '##op', '##ho', '##bia', '’', '–', 'The', 'ta', '##unt', 'of', 'Islam', '##op', '##ho', '##bia', 'is', 'used', 'to', 'silence', 'any', 'criticism', 'of', 'the', 'Islamic', 'world', ',', 'including', 'Islamic', 'ex', '##tre', '##mism', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1135,  1110,  1177, 22564,  1115,  3103,   117,  1980,  1105,\n",
      "          1103,  1646,  1132, 11289,  1111, 16654,  1105,  9480,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'It is so ironic that Israel, Europe and the US are blamed for racism and discrimination', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'It', 'is', 'so', 'ironic', 'that', 'Israel', ',', 'Europe', 'and', 'the', 'US', 'are', 'blamed', 'for', 'racism', 'and', 'discrimination', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2563, 19685,  1158,  8988, 22352,  1118,  4147,  5250,  8619,\n",
      "          5413,  1113,  9786,  1105,   142,  2145,  1183,  1105,  1173,  1274,\n",
      "          3798,  1948,  1106,  1103, 27140,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': 'People exploiting Palestinian solidarity by selling pro Palestine clothing on Amazon and Etsy and then donating money to the IDF.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'People', 'exploit', '##ing', 'Palestinian', 'solidarity', 'by', 'selling', 'pro', 'Palestine', 'clothing', 'on', 'Amazon', 'and', 'E', '##ts', '##y', 'and', 'then', 'don', '##ating', 'money', 'to', 'the', 'IDF', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 5486, 8988, 9891, 1138, 1151, 1841, 1107, 1851, 1552,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '67 Palestinian journalists have been killed in 50 days.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '67', 'Palestinian', 'journalists', 'have', 'been', 'killed', 'in', '50', 'days', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009, 18973,   112,   189,  1155,  1103, 23755,  5620,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Why Aren't all the Palestinians Dead?\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'Aren', \"'\", 't', 'all', 'the', 'Palestinians', 'Dead', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  1248,  3312,  3279,  3234,  1867,  1119,  1108,  2356,\n",
      "           109,  1406,  2107,  1106,  1576,  1222, 24736,  1161,   157, 20737,\n",
      "          1830,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A second Michigan Senate candidate says he was offered $20M to run against Rashida Tlaib', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'second', 'Michigan', 'Senate', 'candidate', 'says', 'he', 'was', 'offered', '$', '20', '##M', 'to', 'run', 'against', 'Rashid', '##a', 'T', '##lai', '##b', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2091,  1128,  1267,  1251,  3154,  1104,   170, 21423,  2322,\n",
      "          1107,  1240,  1583,  1290,  1103,  1594,  1144,  1408,   136, 14425,\n",
      "          1714,  1106,  2934,  1107,  1103,  7640,  1103,  1583,  1128,  1132,\n",
      "          1121,  1105,  1240, 25059,  2480,  1103, 21423,  1110,  3903,  1137,\n",
      "          1136,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Do you see any effects of a boycott campaign in your country since the war has started? Feel free to share in the comments the country you are from and your outlook whether the boycott is effective or not.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Do', 'you', 'see', 'any', 'effects', 'of', 'a', 'boycott', 'campaign', 'in', 'your', 'country', 'since', 'the', 'war', 'has', 'started', '?', 'Feel', 'free', 'to', 'share', 'in', 'the', 'comments', 'the', 'country', 'you', 'are', 'from', 'and', 'your', 'outlook', 'whether', 'the', 'boycott', 'is', 'effective', 'or', 'not', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3046,  4878, 21832,  1105, 23755, 11485,  1112, 14092,  7117,\n",
      "          4973,  2675,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'More Israeli hostages and Palestinians freed as ceasefire extension agreed', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'More', 'Israeli', 'hostages', 'and', 'Palestinians', 'freed', 'as', 'cease', '##fire', 'extension', 'agreed', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1797,  1104,  4878, 14551,  1867,   107,  1422,  1534,   113,\n",
      "          4859,   114,  1608,  1121, 17737,  1107,   170,  1618,  3879,  1190,\n",
      "          1196, 17737,   117,  1256,  1463,  1131,  1225,  1136,  3531,  1103,\n",
      "          5557,   119,   107,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'daughter of Israeli hostage says \"My mother(85) returned from captivity in a better condition than before captivity, even though she did not receive the drugs.\"', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'daughter', 'of', 'Israeli', 'hostage', 'says', '\"', 'My', 'mother', '(', '85', ')', 'returned', 'from', 'captivity', 'in', 'a', 'better', 'condition', 'than', 'before', 'captivity', ',', 'even', 'though', 'she', 'did', 'not', 'receive', 'the', 'drugs', '.', '\"', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  1209,  1309,  1267,  3519,  1235,  1152,  1169, 18986,\n",
      "          1103,  4346,  1152,  2080,  1113, 23755,  2455,  1116,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel will never see peace until they can recognise the sword they hold on Palestinians necks', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'will', 'never', 'see', 'peace', 'until', 'they', 'can', 'recognise', 'the', 'sword', 'they', 'hold', 'on', 'Palestinians', 'neck', '##s', '[SEP]']}, {'title': {'input_ids': tensor([[ 101,  138, 2304, 1121, 1126, 4699,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A question from an Arab', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'question', 'from', 'an', 'Arab', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1398,  1142,  1166,  1126, 10062,  6093,  1107,  1103,  3901,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'All this over an invisible creature in the sky?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'All', 'this', 'over', 'an', 'invisible', 'creature', 'in', 'the', 'sky', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  4806,  3103,  1104, 13275,  1158, 14092,  7117,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas accused Israel of breaching ceasefire.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'accused', 'Israel', 'of', 'breach', '##ing', 'cease', '##fire', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2091,   194,   112,  1155,  1341, 24574,  1541,  1169,   112,\n",
      "           189, 12726,  1103, 21832,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Do y'all think Hamas really can't locate the hostages?\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Do', 'y', \"'\", 'all', 'think', 'Hamas', 'really', 'can', \"'\", 't', 'locate', 'the', 'hostages', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 13280,  1126,  1110, 20439,  2646,   119,  1821,  1161,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'im an israeli. ama', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'im', 'an', 'is', '##rae', '##li', '.', 'am', '##a', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   552, 28457,   539, 28460, 28464, 28452,   546, 28445,   548,\n",
      "         28463, 28452, 28448,   100,   535, 28466, 28450, 28455, 28449,   119,\n",
      "           100,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'עם הנצח לא מפחד מדרך ארוכה. 🇮🇱🇮🇱🇮🇱', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'ע', '##ם', 'ה', '##נ', '##צ', '##ח', 'ל', '##א', 'מ', '##פ', '##ח', '##ד', '[UNK]', 'א', '##ר', '##ו', '##כ', '##ה', '.', '[UNK]', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   786, 17006,  1116,  4461,  3243,  3711,  1115,  3103,  1144,\n",
      "          1841,  4674,  1104, 24574, 17219,   787,   113,  5751,  2371,   114,\n",
      "          2543,  1632,  2593,  1121,   142, 22099, 16809,   117,  1105,   146,\n",
      "          1341,  3114,  1199,  1167, 14222,  1106,  1293,   119,  7149,  4732,\n",
      "           119,  7426,  1129,   170,  1363,  1888,  1106,  2934,  1164,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, 'orig_title': '‘Piers Morgan questions claims that Israel has killed thousands of Hamas terrorists’ (Sky news) Another great response from Eylon Levy, and I think gives some more insight to how. Things operate. Could be a good video to share about', 'label': 0, 'str_tokenized_sentence': ['[CLS]', '‘', 'Pier', '##s', 'Morgan', 'questions', 'claims', 'that', 'Israel', 'has', 'killed', 'thousands', 'of', 'Hamas', 'terrorists', '’', '(', 'Sky', 'news', ')', 'Another', 'great', 'response', 'from', 'E', '##ylon', 'Levy', ',', 'and', 'I', 'think', 'gives', 'some', 'more', 'insight', 'to', 'how', '.', 'Things', 'operate', '.', 'Could', 'be', 'a', 'good', 'video', 'to', 'share', 'about', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,   139, 13225,  2105,  1584,  1482,   117,  1608,  1121,\n",
      "         17737,   117,  2283,  1147,  1266,  3676, 14360,  1150,  5742,  1103,\n",
      "         17219,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The Brodetz children, returned from captivity, meet their family dog Rodney who escaped the terrorists', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'The', 'B', '##rod', '##et', '##z', 'children', ',', 'returned', 'from', 'captivity', ',', 'meet', 'their', 'family', 'dog', 'Rodney', 'who', 'escaped', 'the', 'terrorists', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  1202,  1195,  1221,  1164,  1293,  1103, 21832,  1107,\n",
      "         15109,  1127,  5165,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What do we know about how the hostages in Gaza were treated?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'do', 'we', 'know', 'about', 'how', 'the', 'hostages', 'in', 'Gaza', 'were', 'treated', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2627, 1132, 1199, 1632, 8988, 5094, 1103, 1362, 1431, 2373,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Who are some great Palestinian writers the world should read', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Who', 'are', 'some', 'great', 'Palestinian', 'writers', 'the', 'world', 'should', 'read', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2966,  1103,  7414, 16416,  2197,  2561,  3103,  1137,  1660,\n",
      "          4384,  1103,  2912,  1106,  2561,  3103,  1191,  1152,  1177,  4835,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Did the UN partition plan create Israel or give Jews the ability to create Israel if they so choose?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Did', 'the', 'UN', 'partition', 'plan', 'create', 'Israel', 'or', 'give', 'Jews', 'the', 'ability', 'to', 'create', 'Israel', 'if', 'they', 'so', 'choose', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4596, 11627,  2629,   131,  9092,   787,   188,  1107,  3658,\n",
      "          1110,  1208,  4360,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Boycott effect: McDonald’s in Pakistan is now Muslim', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Boy', '##cott', 'effect', ':', 'McDonald', '’', 's', 'in', 'Pakistan', 'is', 'now', 'Muslim', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   118, 24574,  1594,  1686,   131,  3081, 23755, 11485,\n",
      "          1170,  1429,  4878, 21832,  1308,   132, 15109, 21978,  2925,  1118,\n",
      "          1160,  1552,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel-Hamas war live: 33 Palestinians freed after 11 Israeli hostages released; Gaza truce extended by two days', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', '-', 'Hamas', 'war', 'live', ':', '33', 'Palestinians', 'freed', 'after', '11', 'Israeli', 'hostages', 'released', ';', 'Gaza', 'truce', 'extended', 'by', 'two', 'days', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1406,  1377,  2044,   117, 10423,   117,  1288,  4481,  3072,\n",
      "           117,  8183,  2394, 17426,  1116,  3072,   117, 14162,  2126,  3072,\n",
      "          1105, 13743,  2332,  3380,   119,  1220,  1341,  1128,  1132,  4736,\n",
      "          1536,  1106,  2059,  1142,  1110,  1136, 19643,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '20k dead, 220,000 homes destroyed, 140 media HQs destroyed, 270 schools destroyed and 124 health facilities. They think you are stupid enough to believe this is not genocide.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '20', '##k', 'dead', ',', '220', ',', '000', 'homes', 'destroyed', ',', '140', 'media', 'HQ', '##s', 'destroyed', ',', '270', 'schools', 'destroyed', 'and', '124', 'health', 'facilities', '.', 'They', 'think', 'you', 'are', 'stupid', 'enough', 'to', 'believe', 'this', 'is', 'not', 'genocide', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 3100, 1103, 1646, 1253, 2484, 1114, 3103, 1107, 1476, 1201, 1159,\n",
      "          136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Will the US still stand with Israel in 30 years time?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Will', 'the', 'US', 'still', 'stand', 'with', 'Israel', 'in', '30', 'years', 'time', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  1110,  1103,  2078, 14551,  2190,  9901,  1146,  1114,\n",
      "          1103,  6596,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How is the official hostage list matching up with the releases?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'is', 'the', 'official', 'hostage', 'list', 'matching', 'up', 'with', 'the', 'releases', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7268,  1116,  1111, 14092,   118,  1783,  1107,  1103,  3103,\n",
      "           118, 24574,  1594,   187, 20708,  1331, 13298,  1121,  1756,  1106,\n",
      "          3312,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Calls for cease-fire in the Israel-Hamas war roil city councils from California to Michigan', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Call', '##s', 'for', 'cease', '-', 'fire', 'in', 'the', 'Israel', '-', 'Hamas', 'war', 'r', '##oil', 'city', 'councils', 'from', 'California', 'to', 'Michigan', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  5835,  1209,  1770,  1129,  2756,  1106,  1103,  4878,\n",
      "          1433,  1105,  4878,  2808,   131,  1103,  1836,  1104,  1155,  1103,\n",
      "           170,  1830, 13890,  8870,   117,  1259,  1103,  2803,   118,  1107,\n",
      "          3670,  1111,  1126,  1322,  1106,  1103,  1594,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A proposal will soon be presented to the Israeli government and Israeli society: the release of all the abductees, including the soldiers - in exchange for an end to the war', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'A', 'proposal', 'will', 'soon', 'be', 'presented', 'to', 'the', 'Israeli', 'government', 'and', 'Israeli', 'society', ':', 'the', 'release', 'of', 'all', 'the', 'a', '##b', '##duct', '##ees', ',', 'including', 'the', 'soldiers', '-', 'in', 'exchange', 'for', 'an', 'end', 'to', 'the', 'war', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,   112,   182,  1126,  1110, 20439,  2646, 13964,   117,\n",
      "          6586,  1592,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"I'm an israeli teen, AMA.\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'I', \"'\", 'm', 'an', 'is', '##rae', '##li', 'teen', ',', 'AM', '##A', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,  1821,  1121,  1110, 20439,  1233,   117,  1110,   170,\n",
      "           179,  5773,  1105,   195,  1988,  1776,   117,  2367,  1143,  1625,\n",
      "           106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'I am from israel, is a jew and zionist, ask me anything!', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'I', 'am', 'from', 'is', '##rae', '##l', ',', 'is', 'a', 'j', '##ew', 'and', 'z', '##ion', '##ist', ',', 'ask', 'me', 'anything', '!', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1731, 1109, 1537, 7193, 1116, 3103,  138, 4299, 9157,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How The West Gives Israel A Free Pass', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'How', 'The', 'West', 'Give', '##s', 'Israel', 'A', 'Free', 'Pass', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2453,  1316, 14551,  1118, 24574,  1107, 15109,  2023,  1107,\n",
      "         10096,  1116,   118,  2592,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Women held hostage by Hamas in Gaza kept in cages - report', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Women', 'held', 'hostage', 'by', 'Hamas', 'in', 'Gaza', 'kept', 'in', 'cage', '##s', '-', 'report', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   118,  8619,  1594,   131,  1960,  8988,  1482,  2046,\n",
      "          2044,  1107, 16705,  1394, 15820,  3227,  1107,  1537,  2950,  3379,\n",
      "          2393,   118,   144, 14640,  1233,   117,   130,   117,  1105, 16209,\n",
      "         27040, 20369,  1389,  8158,  2393,   118,   160,  9823,  1161,   117,\n",
      "          1405,   117,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel-Palestine war: Two Palestinian children shot dead in Jenin refugee camp in West Bank Adam al-Ghoul, 9, and Basil Suleiman Abu al-Wafa, 15,', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', '-', 'Palestine', 'war', ':', 'Two', 'Palestinian', 'children', 'shot', 'dead', 'in', 'Jen', '##in', 'refugee', 'camp', 'in', 'West', 'Bank', 'Adam', 'al', '-', 'G', '##hou', '##l', ',', '9', ',', 'and', 'Basil', 'Sul', '##eim', '##an', 'Abu', 'al', '-', 'W', '##af', '##a', ',', '15', ',', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1619,  3103,  1191,  1128,  1920,  1164,  1103,   185,\n",
      "          7745,  7136,  1104, 15109,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why support Israel if you care about the poeple of Gaza?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'support', 'Israel', 'if', 'you', 'care', 'about', 'the', 'p', '##oe', '##ple', 'of', 'Gaza', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8329,   118, 24574,  6000,  1121, 15109,   117, 13735,  1103,\n",
      "          3548,  1115,  3103,   112,   188,  1594,  1209,   107, 20411,  3708,\n",
      "           107,  1103,  1416,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Anti-Hamas voices from Gaza, addressing the claim that Israel\\'s war will \"Radicalize\" the population', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Anti', '-', 'Hamas', 'voices', 'from', 'Gaza', ',', 'addressing', 'the', 'claim', 'that', 'Israel', \"'\", 's', 'war', 'will', '\"', 'Radical', '##ize', '\"', 'the', 'population', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24930,  1883, 13280,  2312,   112,   188,  4893,  1164,  1110,\n",
      "         20439,  1233,   118,  5871,  7941,  1594,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Adel imam's opinion about israel-hamas war \", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Ad', '##el', 'im', '##am', \"'\", 's', 'opinion', 'about', 'is', '##rae', '##l', '-', 'ha', '##mas', 'war', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  6064,  1110, 24574,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Everyone is Hamas.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Everyone', 'is', 'Hamas', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101,  146, 1274,  787,  189, 1341, 1122,  787,  188, 2848, 2217, 9084,\n",
      "         1863,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'I don’t think it’s antisemitism', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'I', 'don', '’', 't', 'think', 'it', '’', 's', 'anti', '##se', '##mit', '##ism', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878, 15919, 12062,   787,  3921,  7667,  2372,  6819,   157,\n",
      "          8167, 13448,  4772,  1650, 15463, 20080, 25430, 18430,  1116,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli Hostages’ Family Members Are Being Threatened By Suspicious Texts', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'Host', '##ages', '’', 'Family', 'Members', 'Are', 'Being', 'T', '##hr', '##eat', '##ened', 'By', 'Su', '##sp', '##icious', 'Text', '##s', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1110,  3103,  9321,  1177,  2213,  1120, 11629,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why is Israel seemingly so bad at PR?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'is', 'Israel', 'seemingly', 'so', 'bad', 'at', 'PR', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 7187, 1103, 1286, 1920, 1164, 4878, 5256,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Does the left care about Israeli victims?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Does', 'the', 'left', 'care', 'about', 'Israeli', 'victims', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 14709,  9297,  1104,  5871,  7941,  1105,  1136,  1119,  1584,\n",
      "         15792, 10358,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Getting rid of hamas and not hezbollah?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Getting', 'rid', 'of', 'ha', '##mas', 'and', 'not', 'he', '##z', '##bol', '##lah', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  1110,  1286,  1104,  1103, 15109, 13899,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What is left of the Gaza Zoo', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'What', 'is', 'left', 'of', 'the', 'Gaza', 'Zoo', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1803,   131, 16899,  2559,  3128,  4294, 14680,  1611, 27440,\n",
      "           117,  1103,  1178,  8988,  1684,  1107,  1103, 16899,  2559,  3608,\n",
      "          1805,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Canada: CTV News fired Yara Jamal, the only Palestinian working in the CTV Atlantic region.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Canada', ':', 'CT', '##V', 'News', 'fired', 'Ya', '##ra', 'Jamal', ',', 'the', 'only', 'Palestinian', 'working', 'in', 'the', 'CT', '##V', 'Atlantic', 'region', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101,  107, 7187, 3103, 1138,  170, 1268, 1106, 4056,  136,  107,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '\"Does Israel have a right to exist?\"', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '\"', 'Does', 'Israel', 'have', 'a', 'right', 'to', 'exist', '?', '\"', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  2466,  2394,  1132,  1594, 13037,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The western media are war criminals', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', 'western', 'media', 'are', 'war', 'criminals', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15109,   112,   188,   107,   152, 19515, 26392,   107,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Gaza\\'s \"Occupation\"', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Gaza', \"'\", 's', '\"', 'O', '##cc', '##upation', '\"', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1109, 2590, 1106, 1103, 4139, 1110, 8988, 1972, 1113, 1103, 3062,\n",
      "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The answer to the conflict is Palestinian education on the truth', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', 'answer', 'to', 'the', 'conflict', 'is', 'Palestinian', 'education', 'on', 'the', 'truth', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  9329,  1212,  8619,  1113,  1130,  8419, 12139,   131,   107,\n",
      "          1109,  1721,  1165,  1103,  2027, 27900,  1233,  8158,   160,  9823,\n",
      "          1161,  1108,  2046,  1105,  1841,  1118,  1103,  4878,  5846,  2088,\n",
      "          1107, 16705,  1394,   119,  1853,   119,  1429,   119,  1695,   588,\n",
      "         28480, 28489, 23525,   565, 28488, 28495, 28475, 28493,   570, 17754,\n",
      "         28496, 18191,   565, 28495, 28475, 28480, 28477, 28495, 28475, 28495,\n",
      "           565, 28495, 17754, 28475, 19775,   583, 28495, 28497,   565, 28495,\n",
      "         28488, 28492, 28495,   565, 28495, 28485, 15389, 16070, 18191,   566,\n",
      "         28475, 28484, 28495,   562, 28476, 28496,   565, 28495, 28496, 28492,\n",
      "         28475,   585, 16070,   570, 17754, 16070, 17754,   119,   107,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}, 'orig_title': 'Eye On Palestine on Instagram\\u200e: \"The moment when the child Bail Abu Wafa was shot and killed by the Israeli occupation forces in Jenin. 29.11.23 لحظة اطلاق جنود الاحتلال النار على الطفل الشهيد باسل أبو الوفا في جنين.\"\\u200e', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Eye', 'On', 'Palestine', 'on', 'In', '##sta', '##gram', ':', '\"', 'The', 'moment', 'when', 'the', 'child', 'Bai', '##l', 'Abu', 'W', '##af', '##a', 'was', 'shot', 'and', 'killed', 'by', 'the', 'Israeli', 'occupation', 'forces', 'in', 'Jen', '##in', '.', '29', '.', '11', '.', '23', 'ل', '##ح', '##ظ', '##ة', 'ا', '##ط', '##ل', '##ا', '##ق', 'ج', '##ن', '##و', '##د', 'ا', '##ل', '##ا', '##ح', '##ت', '##ل', '##ا', '##ل', 'ا', '##ل', '##ن', '##ا', '##ر', 'ع', '##ل', '##ى', 'ا', '##ل', '##ط', '##ف', '##ل', 'ا', '##ل', '##ش', '##ه', '##ي', '##د', 'ب', '##ا', '##س', '##ل', 'أ', '##ب', '##و', 'ا', '##ل', '##و', '##ف', '##ا', 'ف', '##ي', 'ج', '##ن', '##ي', '##ن', '.', '\"', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 27140,   157,   118, 11710, 18274,  3646,  6391,  1535,  1105,\n",
      "          1482,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'IDF T-shirts depict killing pregnant women and children.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'IDF', 'T', '-', 'shirts', 'depict', 'killing', 'pregnant', 'women', 'and', 'children', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 3103, 1144, 3890, 1122, 2425, 1764, 7649,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel has achieved it primary military objective', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'has', 'achieved', 'it', 'primary', 'military', 'objective', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15109,  1179,  1150,  5742,  1106,  3103,   117,  4213,  1106,\n",
      "         14142,  6117,  1117,  1642,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Gazan who escaped to Israel, converted to Judaism shares his story', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Gaza', '##n', 'who', 'escaped', 'to', 'Israel', ',', 'converted', 'to', 'Judaism', 'shares', 'his', 'story', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2563,  1113,  1251,  1334,   131,  1731,  1144,  1142,  4139,\n",
      "          2014,   120, 20968,  1128,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'People on any side: How has this conflict changed/impacted you?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'People', 'on', 'any', 'side', ':', 'How', 'has', 'this', 'conflict', 'changed', '/', 'impacted', 'you', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  2754,  1104,  4789,  1169,  1129,  1434,   117,  1105,\n",
      "          1122,  1169,  1145,  1129,  3442,  9203, 18640,   117,  1105, 23959,\n",
      "          1181,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A culture of resistance can be built, and it can also be methodically dismantled, and sabotaged.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'A', 'culture', 'of', 'resistance', 'can', 'be', 'built', ',', 'and', 'it', 'can', 'also', 'be', 'method', '##ically', 'dismantled', ',', 'and', 'sabotage', '##d', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 5399,  106,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}, 'orig_title': 'Rights!', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Rights', '!', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 14856, 13356,  1988,   117,  1110,  1142,  3103,   787,   188,\n",
      "          1263,  1858,  2197,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Discussion , is this Israel’s long term plan ?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Disc', '##uss', '##ion', ',', 'is', 'this', 'Israel', '’', 's', 'long', 'term', 'plan', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103, 14303,   140, 15432,  8988,  5977,  1116,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel Must Crush Palestinian Hopes', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'Must', 'C', '##rush', 'Palestinian', 'Hope', '##s', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2778, 12066, 17544,  1987,   144,  6639,  1766, 25702,  2744,\n",
      "          7155,  1725,  1195,  1538,  1322,  1103,  5937,  1113, 15109,   117,\n",
      "          1105,  1714,  8619,  1121,  1103, 12800,  5846,  1104,  4878, 23726,\n",
      "          1116,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Jewish Holocaust survivor Dr Gabor Maté explains why we must end the assault on Gaza, and free Palestine from the brutal occupation of Israeli Zionists.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Jewish', 'Holocaust', 'survivor', 'Dr', 'G', '##ab', '##or', 'Mat', '##é', 'explains', 'why', 'we', 'must', 'end', 'the', 'assault', 'on', 'Gaza', ',', 'and', 'free', 'Palestine', 'from', 'the', 'brutal', 'occupation', 'of', 'Israeli', 'Zionist', '##s', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7414,  2069, 11840, 15549, 15109,  1416,  1106,  7977,  4878,\n",
      "         20854,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'UNRWA updates Gaza population to reflect Israeli expulsion?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'UN', '##R', '##WA', 'updates', 'Gaza', 'population', 'to', 'reflect', 'Israeli', 'expulsion', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  1202, 23726,  1116,  1328,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What do Zionists want?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'do', 'Zionist', '##s', 'want', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 20820, 18266, 23222,  1867,  1103,  3589,  1226,  1149,  4632,\n",
      "          1105,  3301,  1117,  1677,  1268,  7179,  1115,  1119,  2041,  1110,\n",
      "          1184,   112,   188,  7202,   170,  8988,  1352,   119,  1188,  2762,\n",
      "           112,   189,  1103,  1148,  1159, 20820, 18266, 23222,  1144, 12418,\n",
      "         12165,  1164, 10878,   170,  1160,  1352,  3519,  3311,   117,  1105,\n",
      "          1115,  1119,  1169,  3253, 19109,  1470,  4893,  1107,  1738,  1114,\n",
      "          1117,  2887,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Netanyahu says the quiet part out loud and tells his far right voters that he alone is what's stopping a Palestinian state. This isn't the first time Netanyahu has bragged about preventing a two state peace agreement, and that he can easily manipulate public opinion in America with his lies.\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Net', '##any', '##ahu', 'says', 'the', 'quiet', 'part', 'out', 'loud', 'and', 'tells', 'his', 'far', 'right', 'voters', 'that', 'he', 'alone', 'is', 'what', \"'\", 's', 'stopping', 'a', 'Palestinian', 'state', '.', 'This', 'isn', \"'\", 't', 'the', 'first', 'time', 'Net', '##any', '##ahu', 'has', 'bra', '##gged', 'about', 'preventing', 'a', 'two', 'state', 'peace', 'agreement', ',', 'and', 'that', 'he', 'can', 'easily', 'manipulate', 'public', 'opinion', 'in', 'America', 'with', 'his', 'lies', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5096,   118, 22572,  7363,  1116,  2520,  1553,   789,  4878,\n",
      "          6977,  1841,  4384,  1120,  1103,  1390,  3782,  1113, 14125,   128,\n",
      "           790,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Pro-chamas talking point “Israeli tanks killed Jews at the music festival on Oct 7”', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Pro', '-', 'ch', '##ama', '##s', 'talking', 'point', '“', 'Israeli', 'tanks', 'killed', 'Jews', 'at', 'the', 'music', 'festival', 'on', 'Oct', '7', '”', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 10325,  1130,  5062,  1111,  3875, 15386,  1370, 23755,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Concert In Dublin for Medical Aid For Palestinians', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Concert', 'In', 'Dublin', 'for', 'Medical', 'Aid', 'For', 'Palestinians', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1132,  1177,  1242,  1104,  1103, 21832,  9808,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why are so many of the hostages elderly?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'are', 'so', 'many', 'of', 'the', 'hostages', 'elderly', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1706,  4878,  1116,  1114,  6210,  7564,   117,  1293,  1127,\n",
      "          1128,   120,  1240,  2153,   120,  1240, 15313,  5165,  1107,  4498,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'To Israelis with Egyptian origins, how were you/your parents/ your grandparents treated in Egypt?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'To', 'Israeli', '##s', 'with', 'Egyptian', 'origins', ',', 'how', 'were', 'you', '/', 'your', 'parents', '/', 'your', 'grandparents', 'treated', 'in', 'Egypt', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 6301, 1104, 5871, 7941, 4598, 1105, 3646,  170,  130, 1201, 1385,\n",
      "         2298,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Video of hamas shooting and killing a 9 years old boy', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Video', 'of', 'ha', '##mas', 'shooting', 'and', 'killing', 'a', '9', 'years', 'old', 'boy', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  4810, 19299,  4068,  6102,  1104, 17823,  1166, 24574,\n",
      "          2035,   197,  3103,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli intelligence leak details extent of warnings over Hamas attack | Israel', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'intelligence', 'leak', 'details', 'extent', 'of', 'warnings', 'over', 'Hamas', 'attack', '|', 'Israel', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 17554,   787,   189,  2187,  1150,  1128,  1619,   118,  1241,\n",
      "          1132, 18970,  4719,  1234,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Doesn’t matter who you support- both are pathetic evil people', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Doesn', '’', 't', 'matter', 'who', 'you', 'support', '-', 'both', 'are', 'pathetic', 'evil', 'people', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  6963,  4878,  3955,   131,   786,  1422,  1433,  1105,  1764,\n",
      "          1132,  1594, 13037,   787,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Former Israeli pilot: ‘My government and military are war criminals’', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Former', 'Israeli', 'pilot', ':', '‘', 'My', 'government', 'and', 'military', 'are', 'war', 'criminals', '’', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  2892,  1105, 25030,  1158,  1104, 21863,  1863,   117,\n",
      "         16409, 18220,  1174,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The History and Meaning of Zionism, Explained', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'The', 'History', 'and', 'Mean', '##ing', 'of', 'Zion', '##ism', ',', 'Ex', '##plain', '##ed', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2295,   127,   131,   154, 11192,  2312,  4292,  1116,  1836,\n",
      "          1160,  1938, 21832,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Day 6: Qassam Brigades release two Russian hostages.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Day', '6', ':', 'Q', '##ass', '##am', 'Brigade', '##s', 'release', 'two', 'Russian', 'hostages', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1136,  1198,  2760,  1106,  1321,  1167, 21832,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why not just continue to take more hostages?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'not', 'just', 'continue', 'to', 'take', 'more', 'hostages', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4766,  1357, 24574, 20507, 23725,  5960, 12013,  3382,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '7th October Hamas Massacre Survivor Testimony', 'label': -1, 'str_tokenized_sentence': ['[CLS]', '7th', 'October', 'Hamas', 'Massacre', 'Survivor', 'Test', '##imo', '##ny', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2966, 24574,  5363,  3103,  1106,  1231,  6163, 15045,  1107,\n",
      "          1142,  4633,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Did Hamas expect Israel to retaliate in this fashion?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Did', 'Hamas', 'expect', 'Israel', 'to', 're', '##tal', '##iate', 'in', 'this', 'fashion', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 8988, 1657, 7251, 1107, 1103, 1314, 1620, 1201,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinian land stolen in the last 100 years.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinian', 'land', 'stolen', 'in', 'the', 'last', '100', 'years', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1422, 13085,  1116,  1112,   170,  2778,  1237,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'My Feelings as a Jewish American', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'My', 'Feeling', '##s', 'as', 'a', 'Jewish', 'American', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1220, 10566,  1412,  1657,   117,  1208,  1152,  1328,  1412,\n",
      "          2754,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'They stole our land, now they want our culture', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'They', 'stole', 'our', 'land', ',', 'now', 'they', 'want', 'our', 'culture', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1132,  8988,   107, 11375,  1468,   107, 12730,  1229,\n",
      "          4878, 21832,  1132,  1218,  5165,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why are Palestinian \"Prisoners\" tortured while Israeli hostages are well treated?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'are', 'Palestinian', '\"', 'Prison', '##ers', '\"', 'tortured', 'while', 'Israeli', 'hostages', 'are', 'well', 'treated', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 11930,  6888, 14551,  1867,  4878,  1116,  1316,  1114,  1140,\n",
      "          1127,  7425,  1114,  3651, 16886,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Released Thai hostage says Israelis held with him were beaten with electric cables', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Released', 'Thai', 'hostage', 'says', 'Israeli', '##s', 'held', 'with', 'him', 'were', 'beaten', 'with', 'electric', 'cables', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  1242, 21832,  1127,  1678,   136, 14709,  3216,  6615,\n",
      "           106,   113, 11866,   136, 11202,   136,  4805,   136,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How many hostages were taken? Getting mixed answers! (230? 240? 250?)', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'many', 'hostages', 'were', 'taken', '?', 'Getting', 'mixed', 'answers', '!', '(', '230', '?', '240', '?', '250', '?', ')', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,  1189,   170,  2190,  1104,  1103,  5251,  1104, 19643,\n",
      "          1114,  5136,  1121,  8619,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'I made a list of the stages of genocide with examples from Palestine', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'I', 'made', 'a', 'list', 'of', 'the', 'stages', 'of', 'genocide', 'with', 'examples', 'from', 'Palestine', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 11679,  8674,  1158,  1888,  2196, 24505, 12237,  2044,  1107,\n",
      "         15109,  2704,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Harrowing video shows premature babies dead in Gaza hospital', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Ha', '##rrow', '##ing', 'video', 'shows', 'premature', 'babies', 'dead', 'in', 'Gaza', 'hospital', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   107,  8329,   118, 21863,  1863,   107,  1110,   170,  1167,\n",
      "         12477,  2646, 15454,  1532,  1104,  2848,  2217,  9084,  1863,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '\"Anti-Zionism\" is a more malignant form of antisemitism', 'label': -1, 'str_tokenized_sentence': ['[CLS]', '\"', 'Anti', '-', 'Zion', '##ism', '\"', 'is', 'a', 'more', 'ma', '##li', '##gnant', 'form', 'of', 'anti', '##se', '##mit', '##ism', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  9743,  1234,  1121,  1168,  2182,  1150,  1132,  1136,  2626,\n",
      "          2272,  1106,  1142,  1594,   117,  1885,  1122,  1205,   170, 23555,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Should people from other countries who are not directly related to this war, turn it down a notch ?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Should', 'people', 'from', 'other', 'countries', 'who', 'are', 'not', 'directly', 'related', 'to', 'this', 'war', ',', 'turn', 'it', 'down', 'a', 'notch', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1542, 23181,  1617,   117, 27161,  1182, 15820,  3227,  1107,\n",
      "          1103,  1745,  3085,   117,  1175,  1110,  1185, 24574,   117,  4040,\n",
      "         18437,   113,  2894,   126,  1377,  1306,  2079,   114,   117,  1133,\n",
      "          1175,  1108,  1253,  4878,  6977,  2935,  4067,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '17 Sep 2002, Amari refugee camp in the west bank, there is no Hamas, nor rockets (beyond 5km range), but there was still Israeli tanks fighting kids.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '17', 'Sep', '2002', ',', 'Amar', '##i', 'refugee', 'camp', 'in', 'the', 'west', 'bank', ',', 'there', 'is', 'no', 'Hamas', ',', 'nor', 'rockets', '(', 'beyond', '5', '##k', '##m', 'range', ')', ',', 'but', 'there', 'was', 'still', 'Israeli', 'tanks', 'fighting', 'kids', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1130,  1103,  1537,  2950,   117, 17443,  1104, 11375,  1468,\n",
      "          7786,  5026,  8704,  1111, 24574,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'In the West Bank, Release of Prisoners Deepens Support for Hamas', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'In', 'the', 'West', 'Bank', ',', 'Release', 'of', 'Prison', '##ers', 'Deep', '##ens', 'Support', 'for', 'Hamas', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2372,  1175,  1251, 15549,  1113,  1103, 22586,  4878,  1590,\n",
      "          1107,  1103, 16358, 14791, 16877, 24574,  1888,  1114,  1892,  1113,\n",
      "          1123,  6023,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Are there any updates on the abducted Israeli woman in the horrifying Hamas video with blood on her pants?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Are', 'there', 'any', 'updates', 'on', 'the', 'abducted', 'Israeli', 'woman', 'in', 'the', 'ho', '##rri', '##fying', 'Hamas', 'video', 'with', 'blood', 'on', 'her', 'pants', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   113, 24318, 27551,   114, 22718,  1104, 23637,  1141,  1104,\n",
      "          1103,  1160,  1482,  1107, 10922,  9561,  1118,   146,  2346,  2271,\n",
      "          4791,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '(Graphic Content) Moments of murdering one of the two children in Jineen by IOF Forces', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '(', 'Graphic', 'Content', ')', 'Moments', 'of', 'murdering', 'one', 'of', 'the', 'two', 'children', 'in', 'Jin', '##een', 'by', 'I', '##O', '##F', 'Forces', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731, 19016, 10147,  1658, 14644,  1174,  1157,  1236,  1154,\n",
      "          1217,   170,  3641, 19220,  9127,   117, 17400,  1103,  1470, 14222,\n",
      "          1154,  1157, 14644,  1105,  2848,   118,  9327,  2933,  1113,  1646,\n",
      "          8673,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How AIPAC corrupted its way into being a tax exempt entity, denying the public insight into its corrupt and anti-democratic influence on US politicians', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'How', 'AI', '##PA', '##C', 'corrupt', '##ed', 'its', 'way', 'into', 'being', 'a', 'tax', 'exempt', 'entity', ',', 'denying', 'the', 'public', 'insight', 'into', 'its', 'corrupt', 'and', 'anti', '-', 'democratic', 'influence', 'on', 'US', 'politicians', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2181, 16520, 24574,   170, 13142,  1603,  1858,  2273,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Is eliminating Hamas a realistic short term goal ?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Is', 'eliminating', 'Hamas', 'a', 'realistic', 'short', 'term', 'goal', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  6074,  8988,  2027,  7396,  1107,  1103,  2239,  1108,\n",
      "          1107,   170, 19737,  1111,   170,  2006,  1989,  1170,  1103,   146,\n",
      "          2346,  2271,  3950,  1140,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}, 'orig_title': 'The youngest Palestinian child prisoner in the deal was in a coma for a whole week after the IOF arrested him', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', 'youngest', 'Palestinian', 'child', 'prisoner', 'in', 'the', 'deal', 'was', 'in', 'a', 'coma', 'for', 'a', 'whole', 'week', 'after', 'the', 'I', '##O', '##F', 'arrested', 'him', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 21863, 19888,  1116,  1309, 14092,  1106, 13155,  1103,  1362,\n",
      "          1114,  1147,  1594,  6969,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Zionazis never cease to disgust the world with their war crimes', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Zion', '##azi', '##s', 'never', 'cease', 'to', 'disgust', 'the', 'world', 'with', 'their', 'war', 'crimes', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 13063,  4165, 13541,  3470,  1104, 24505, 12237,  1276,  1107,\n",
      "         15109,   787,   188,  2586, 11896,  1116,  1197,  4288,   787,   188,\n",
      "          3355,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Decomposed bodies of premature babies found in Gaza’s Al Nasr Children’s Hospital', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Dec', '##om', '##posed', 'bodies', 'of', 'premature', 'babies', 'found', 'in', 'Gaza', '’', 's', 'Al', 'Na', '##s', '##r', 'Children', '’', 's', 'Hospital', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,  1821,  1103,  7254,  1104,   170, 16358, 27089, 25134,\n",
      "          1204, 17544,  1150,  6192,  1860,   119,  1327,   112,   188,  5664,\n",
      "          1107, 15109,  1110, 19643,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, 'orig_title': \"I am the grandson of a holocaust survivor who fled Germany. What's happening in Gaza is genocide.\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'I', 'am', 'the', 'grandson', 'of', 'a', 'ho', '##loc', '##aus', '##t', 'survivor', 'who', 'fled', 'Germany', '.', 'What', \"'\", 's', 'happening', 'in', 'Gaza', 'is', 'genocide', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   139, 26859,  1105,  8499,  7193,  3103,   139, 27339, 23114,\n",
      "          1706,  3291,  6262,  2875,  1414, 10725,  1116,   197,  2695,  4765,\n",
      "          1619,  3103,   782,  1185,  2187,  1184,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': 'Biden and Trump Give Israel Blank Check To Commit War Crimes | Both candidates support Israel – no matter what', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'B', '##iden', 'and', 'Trump', 'Give', 'Israel', 'B', '##lank', 'Check', 'To', 'Co', '##mm', '##it', 'War', 'Crime', '##s', '|', 'Both', 'candidates', 'support', 'Israel', '–', 'no', 'matter', 'what', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 20452, 21631,  1200, 19603, 10755,  3279,  4055,  1113,  4703,\n",
      "          2848,  2217,  9084,  1863,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Schumer delivers landmark Senate speech on rising antisemitism', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Sc', '##hum', '##er', 'delivers', 'landmark', 'Senate', 'speech', 'on', 'rising', 'anti', '##se', '##mit', '##ism', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1202,  1177,  1242,  1104,  1128,  9762,  1115,  1103,\n",
      "          1346, 23726,  1116,  6290,  1884,  4934,  2200,  8619,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why do so many of you deny that the early Zionists literally colonized Palestine?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'do', 'so', 'many', 'of', 'you', 'deny', 'that', 'the', 'early', 'Zionist', '##s', 'literally', 'co', '##lon', '##ized', 'Palestine', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,   112,   188,  1103,  1514,  3719,  1206,  1103,  4878,\n",
      "          1286,  1105,  1268,  1112,  1677,  1112, 23755,  1132,  4264,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"What's the main difference between the Israeli left and right as far as Palestinians are concerned?\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', \"'\", 's', 'the', 'main', 'difference', 'between', 'the', 'Israeli', 'left', 'and', 'right', 'as', 'far', 'as', 'Palestinians', 'are', 'concerned', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2577,  1103,  5335, 21978,   117,  3103, 20812,  1142,  1299,\n",
      "           787,   188,  2072,  1266,  1106,  1473,   119,  1230,  1797,  1108,\n",
      "          1260, 25265, 13512,  1118,  1103,  5985,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}, 'orig_title': 'Before the temporary truce, Israel bombed this man’s entire family to death. His daughter was decapitated by the bomb.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Before', 'the', 'temporary', 'truce', ',', 'Israel', 'bombed', 'this', 'man', '’', 's', 'entire', 'family', 'to', 'death', '.', 'His', 'daughter', 'was', 'de', '##cap', '##itated', 'by', 'the', 'bomb', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 8988, 1401, 4961, 6040, 1104, 1117, 1685, 1488, 1118, 4878, 2088,\n",
      "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinian father documents arrest of his young son by Israeli forces', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinian', 'father', 'documents', 'arrest', 'of', 'his', 'young', 'son', 'by', 'Israeli', 'forces', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  2091,  1192, 14425,  3517,  1109,  4714, 21863,  1863,\n",
      "          2181, 17627,  1706, 13704,  4384,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How Do You Feel About The Way Zionism Is Used To Attack Jews?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'How', 'Do', 'You', 'Feel', 'About', 'The', 'Way', 'Zion', '##ism', 'Is', 'Used', 'To', 'Attack', 'Jews', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  1286,  1110, 26375,  1158,  1251,  2810,  1111,   170,\n",
      "          8988,  1352,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The left is dooming any hope for a Palestinian state', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', 'left', 'is', 'doom', '##ing', 'any', 'hope', 'for', 'a', 'Palestinian', 'state', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  3173,  1253,  4009,  1164, 24574,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why BBC still lying about Hamas?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'BBC', 'still', 'lying', 'about', 'Hamas', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15919,  2553,  1105, 24574,  5176,  1245,   171, 22940,  1905,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hostage and Hamas soldier became buddies', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Host', '##age', 'and', 'Hamas', 'soldier', 'became', 'b', '##udd', '##ies', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   789, 19826,  1473,   790,   131,  3103,  4621,  9534,  3653,\n",
      "          1107,  1103, 15109, 19643,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '“Slow death”: Israel weaponizes disease in the Gaza genocide', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '“', 'Slow', 'death', '”', ':', 'Israel', 'weapon', '##izes', 'disease', 'in', 'the', 'Gaza', 'genocide', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2091,  1128,  1341,  1170, 24574,  1764,  3211, 16121,  1144,\n",
      "          1151,  3072,  1115, 27140,  1431,  1301,  1170,  1124,  1584, 15792,\n",
      "         10358,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Do you think after Hamas military capacity hopefully has been destroyed that IDF should go after Hezbollah?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Do', 'you', 'think', 'after', 'Hamas', 'military', 'capacity', 'hopefully', 'has', 'been', 'destroyed', 'that', 'IDF', 'should', 'go', 'after', 'He', '##z', '##bol', '##lah', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2966, 1234, 5042, 1164,  128,  120, 1275,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Did people forget about 7/10?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Did', 'people', 'forget', 'about', '7', '/', '10', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146, 19967,  1193,  1328,  2256,  1105,  2490,  1150, 10021,\n",
      "          4554,  2050, 17054,  1116,  1111,  1147,  4789,  1106, 24692,  1107,\n",
      "          2630,   117,  4554,  2050, 17054,  1116,  1138,  1151,  2157,  1103,\n",
      "          2335,  1260, 18299,  6856,  1105,  5237,  4044,  4253,  1104,   176,\n",
      "         10961,  1161,  1111,  7536,  1108,  1103,  1322,  2273,  1111,  4397,\n",
      "          1105,  8582,  2475,  1172,  1105,  1208,  1122,  1144,  1640,  1408,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'I sincerely want anyone and everyone who condemned palestinians for their resistance to rot in hell, palestinians have been saying the complete depopulation and ethnic cleansing of gaza for settlements was the end goal for decades and nobody believed them and now it has already started.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'I', 'sincere', '##ly', 'want', 'anyone', 'and', 'everyone', 'who', 'condemned', 'pale', '##st', '##inian', '##s', 'for', 'their', 'resistance', 'to', 'rot', 'in', 'hell', ',', 'pale', '##st', '##inian', '##s', 'have', 'been', 'saying', 'the', 'complete', 'de', '##pop', '##ulation', 'and', 'ethnic', 'clean', '##sing', 'of', 'g', '##az', '##a', 'for', 'settlements', 'was', 'the', 'end', 'goal', 'for', 'decades', 'and', 'nobody', 'believed', 'them', 'and', 'now', 'it', 'has', 'already', 'started', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8371,   118,  1214,   118,  1385,  2298,  1621,  1300,  2103,\n",
      "          2044,  1107,  4878,  8818,  1113, 16705,  1394,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Eight-year-old boy among four reported dead in Israeli raid on Jenin', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Eight', '-', 'year', '-', 'old', 'boy', 'among', 'four', 'reported', 'dead', 'in', 'Israeli', 'raid', 'on', 'Jen', '##in', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  1116,  1132,  1315, 24362,  1115,  8988,  4789,  1505,\n",
      "          1482, 12705,  1113,  4632, 20080, 23783,  1468,  1106, 19615,  1105,\n",
      "          8795,  3708,  1172,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israelis are too coward that Palestinian resistance play children conversations on loudspeakers to lure and neutralize them', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', '##s', 'are', 'too', 'coward', 'that', 'Palestinian', 'resistance', 'play', 'children', 'conversations', 'on', 'loud', '##sp', '##eak', '##ers', 'to', 'lure', 'and', 'neutral', '##ize', 'them', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  1202,  1128,  2239,  1114, 11181,  1105,  1286,  1776,\n",
      "          2053,  1150,  4819,  3103,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How do you deal with socialist and leftist friends who hate Israel?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'How', 'do', 'you', 'deal', 'with', 'socialist', 'and', 'left', '##ist', 'friends', 'who', 'hate', 'Israel', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1132,  1103,  5250,   118, 23755,  1177,  2817,  5591,\n",
      "          1113,   789,  1293,  1363,   790, 24574,  1110, 12770, 21832,   136,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why are the pro-Palestinians so focussed on “how good” Hamas is treating hostages??', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'are', 'the', 'pro', '-', 'Palestinians', 'so', 'focus', '##sed', 'on', '“', 'how', 'good', '”', 'Hamas', 'is', 'treating', 'hostages', '?', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,   146,  5195,   131,   138,   128,  1214,   118,  1385,\n",
      "           787,   188,  1297,  1107, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How I survive: A 7 year-old’s life in Gaza', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'How', 'I', 'survive', ':', 'A', '7', 'year', '-', 'old', '’', 's', 'life', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   153,  1643,  1233,  6675,  1164,  3103,  7046,  1207,  7536,\n",
      "         25338,  1233,   117,  1274,   787,   189,  1176,  1122,   136,  1790,\n",
      "           787,   189,  1838,   170,  1594,  1114,  1103,  2202,  3007,  1104,\n",
      "         19643,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Ppl crying about Israel establishing new settlements lol, don’t like it? Don’t start a war with the stated purpose of genocide.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'P', '##p', '##l', 'crying', 'about', 'Israel', 'establishing', 'new', 'settlements', 'lo', '##l', ',', 'don', '’', 't', 'like', 'it', '?', 'Don', '’', 't', 'start', 'a', 'war', 'with', 'the', 'stated', 'purpose', 'of', 'genocide', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5096,   118,  8619, 14339, 16358,  6775,  2073,  1104, 24574,\n",
      "          5256,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Pro-Palestine protesters hound families of Hamas victims.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Pro', '-', 'Palestine', 'protesters', 'ho', '##und', 'families', 'of', 'Hamas', 'victims', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  2088,  4162, 16705,  1394,   117,  2311,  1160,  3287,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli forces storm Jenin, kill two boys.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'forces', 'storm', 'Jen', '##in', ',', 'kill', 'two', 'boys', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  6596,  8145,  1104,  4584, 15817,  1104, 21832,  1217,\n",
      "          1308,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas releases footage of 6th batch of hostages being released', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'releases', 'footage', 'of', '6th', 'batch', 'of', 'hostages', 'being', 'released', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8329,   118, 21863,  1863,  1112,  8329,  2217,  9084,  1863,\n",
      "           131,  1731,  8329,   118, 23726,  6828,  1121,  1103,  8123,  1105,\n",
      "          4114,   159, 18575, 16847,  4384,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, 'orig_title': 'Anti-Zionism as Antisemitism: How Anti-Zionist Language from the Left and Right Vilifies Jews', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Anti', '-', 'Zion', '##ism', 'as', 'Anti', '##se', '##mit', '##ism', ':', 'How', 'Anti', '-', 'Zionist', 'Language', 'from', 'the', 'Left', 'and', 'Right', 'V', '##ili', '##fies', 'Jews', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  6913,  1104, 24574, 12556,  3202,  1830, 13583,  1192, 21762,\n",
      "          2228,   170,  1897,  6122,  4195,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Son of Hamas Mosab Hassan Yousef makes a rather extreme statement', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Son', 'of', 'Hamas', 'Mo', '##sa', '##b', 'Hassan', 'You', '##sef', 'makes', 'a', 'rather', 'extreme', 'statement', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1132,  1234,  3753,  1128,  1243,  3950,   120,  2046,\n",
      "          1111,   188,  1979,  1158,  5753,  1105,   182, 12805, 25918, 23305,\n",
      "          1116,  1120,  1103, 27140,   136,  1130,  1139,  1583,  1754,   117,\n",
      "          1191,  1195,  3885,  5753,  1137,   182, 12805, 25918, 23305,  1116,\n",
      "          1120,  2021,  1195,  1156,  1719,  1243,  2046,  1137,  1301,  2632,\n",
      "          1106,  7237,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why are people surprised you get arrested / shot for slinging rocks and molotov cocktails at the IDF? In my country Australia, if we threw rocks or molotov cocktails at police we would either get shot or go straight to jail.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'are', 'people', 'surprised', 'you', 'get', 'arrested', '/', 'shot', 'for', 's', '##ling', '##ing', 'rocks', 'and', 'm', '##olo', '##tov', 'cocktail', '##s', 'at', 'the', 'IDF', '?', 'In', 'my', 'country', 'Australia', ',', 'if', 'we', 'threw', 'rocks', 'or', 'm', '##olo', '##tov', 'cocktail', '##s', 'at', 'police', 'we', 'would', 'either', 'get', 'shot', 'or', 'go', 'straight', 'to', 'jail', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   787,   188,   139,  5082,  6163,  1785,  2181,  1457,\n",
      "         21420,  2118,  1103,   146,  1918, 10533,  1891,  1104,  1726,   112,\n",
      "           188,  8040,  4114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Israel’s Brutality Is Stoking the Imagination of India's Far Right\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', '’', 's', 'B', '##ru', '##tal', '##ity', 'Is', 'St', '##oki', '##ng', 'the', 'I', '##ma', '##gin', '##ation', 'of', 'India', \"'\", 's', 'Far', 'Right', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  1153, 13622,  1105,  9924,  1116,  8680,  1170, 27669,\n",
      "          1496,  1106,  1103,  1594,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli Shekel and Stocks recover after dipping due to the war', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'She', '##kel', 'and', 'Stock', '##s', 'recover', 'after', 'dipping', 'due', 'to', 'the', 'war', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 4384,  117, 6979, 1105, 7682, 1155, 1338, 1487, 1111,  170, 4299,\n",
      "         8619, 1107, 4343, 2779,  117, 1375, 2201,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Jews, Muslims and Christians all came together for a Free Palestine in Cape Town, South Africa', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Jews', ',', 'Muslims', 'and', 'Christians', 'all', 'came', 'together', 'for', 'a', 'Free', 'Palestine', 'in', 'Cape', 'Town', ',', 'South', 'Africa', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  6596,  3971, 15817,  1104,  4878, 21832,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas releases sixth batch of Israeli hostages', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'releases', 'sixth', 'batch', 'of', 'Israeli', 'hostages', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  2315,  1389,  8790,  1733,  1182,  4149,   131,   112,\n",
      "         27328,  1468,  1111,  8619,   112,  2196,  1293,  4736,  1412,  2808,\n",
      "          1110,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Ayaan Hirsi Ali: 'Queers for Palestine' shows how stupid our society is\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'A', '##ya', '##an', 'Hi', '##rs', '##i', 'Ali', ':', \"'\", 'Que', '##ers', 'for', 'Palestine', \"'\", 'shows', 'how', 'stupid', 'our', 'society', 'is', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 26339,   117,  5528,  1133,  1714,  1208,   119,  8988,  2298,\n",
      "         19120,  6704,  1107,  4878,  3315,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hurt, scared but free now. Palestinian boy recalls abuse in Israeli prison', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Hurt', ',', 'scared', 'but', 'free', 'now', '.', 'Palestinian', 'boy', 'recalls', 'abuse', 'in', 'Israeli', 'prison', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8988,  5985,   118, 18833,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinian bomb-shelters', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinian', 'bomb', '-', 'shelters', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7187,  3103,  1256, 10026,  1106,  1129,  7398,  1112,   170,\n",
      "         11582,  1583,   136,  1188,  1110, 26651,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Does Israel even deserve to be recognised as a legitimate country? This is horrific.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Does', 'Israel', 'even', 'deserve', 'to', 'be', 'recognised', 'as', 'a', 'legitimate', 'country', '?', 'This', 'is', 'horrific', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7178,   131, 19016, 10147,  1658,  1105,  3103,  1654,  1103,\n",
      "          1646,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Report: AIPAC and Israel control the US', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Report', ':', 'AI', '##PA', '##C', 'and', 'Israel', 'control', 'the', 'US', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1130, 26966,  4067,  1105,  9112,  1217,  6169,  1106,  5241,\n",
      "          7910,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Injured kids and civilians being rushed to emergency ward', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'In', '##jured', 'kids', 'and', 'civilians', 'being', 'rushed', 'to', 'emergency', 'ward', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1203,  1321,   131,  1103,  1372,  1115, 17705,  1103,  1211,\n",
      "          1106,  9011,  1431,  1712,  1103,  1657,   113,  3103,   114,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'New take: the group that contributes the most to humanity should keep the land (Israel).', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'New', 'take', ':', 'the', 'group', 'that', 'contributes', 'the', 'most', 'to', 'humanity', 'should', 'keep', 'the', 'land', '(', 'Israel', ')', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1327, 5940, 1106, 1343, 1150, 1274,  112,  189, 2939,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"What happens to those who don't die\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'happens', 'to', 'those', 'who', 'don', \"'\", 't', 'die', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731, 19016, 10147,  1658,  1759,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How AIPAC works', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'AI', '##PA', '##C', 'works', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 5718, 4878, 1116, 1129, 1501, 1106, 1103, 1911, 1104, 1781, 1554,\n",
      "          118, 3418, 1764, 2805, 1222, 7940, 1107, 1103, 1954, 1594,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Would Israelis be open to the idea of taking full-scale military operation against Lebanon in the current war?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Would', 'Israeli', '##s', 'be', 'open', 'to', 'the', 'idea', 'of', 'taking', 'full', '-', 'scale', 'military', 'operation', 'against', 'Lebanon', 'in', 'the', 'current', 'war', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2122,  3225,  8736, 15094,  1370,  8619,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Canadian Health Workers Rally For Palestine', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Canadian', 'Health', 'Workers', 'Rally', 'For', 'Palestine', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  9234,  2405,   117,  1237,  4391,   117,  1113, 27140,  2803,\n",
      "          3351,   107,   122,  2046,   123,  8567,   107, 11710,  4000,  6391,\n",
      "          1535,  1107,  6658,  2771, 10390,  1197,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': 'Abby Martin, American journalist, on IDF soldiers wearing \"1 shot 2 kills\" shirts showing pregnant women in rifle crosshair', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Abby', 'Martin', ',', 'American', 'journalist', ',', 'on', 'IDF', 'soldiers', 'wearing', '\"', '1', 'shot', '2', 'kills', '\"', 'shirts', 'showing', 'pregnant', 'women', 'in', 'rifle', 'cross', '##hai', '##r', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  2778,  1825,  1104, 15323,  8917,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A Jewish person of conscience speaks', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'Jewish', 'person', 'of', 'conscience', 'speaks', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 13297,  8988,  4878, 18959,  1181,  8305,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Cool Palestinian Israeli Podcast', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Cool', 'Palestinian', 'Israeli', 'Po', '##d', '##cast', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  1344, 21050,  1200,  1303,   117,  1328,  1106,  2590,\n",
      "          1251,  3243,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli highschooler here, want to answer any questions.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'high', '##school', '##er', 'here', ',', 'want', 'to', 'answer', 'any', 'questions', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 8640, 1202, 1128, 1341, 2778, 1234, 1435, 1121,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Were do you think Jewish people come from?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Were', 'do', 'you', 'think', 'Jewish', 'people', 'come', 'from', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8640,  1103,  4384,  3155,  1106,  3074,  1111,  1103, 27664,\n",
      "          1196,  3610,  1106,  3103,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Were the Jews supposed to wait for the Messiah before returning to Israel?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Were', 'the', 'Jews', 'supposed', 'to', 'wait', 'for', 'the', 'Messiah', 'before', 'returning', 'to', 'Israel', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2181,   107,  1121,  1103,  2186,  1106,  1103,  2343,   107,\n",
      "          1256,   170, 13142,  5072,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Is \"from the river to the sea\" even a realistic solution', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Is', '\"', 'from', 'the', 'river', 'to', 'the', 'sea', '\"', 'even', 'a', 'realistic', 'solution', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1994,  3799,   131,   107, 14434,  2491,   107,  1105,   107,\n",
      "          1276,  2207,   107,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Washington Post: \"fragile lives\" and \"found ended\"', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Washington', 'Post', ':', '\"', 'fragile', 'lives', '\"', 'and', '\"', 'found', 'ended', '\"', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  2144,   787,   189,  3103,  3076,  1146,  1103,   151,\n",
      "         27487,  1964,  6941,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why doesn’t Israel build up the Negev desert ?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'doesn', '’', 't', 'Israel', 'build', 'up', 'the', 'N', '##ege', '##v', 'desert', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   789,  4299,  8619,   790,  2762,   787,   189, 17575,  1193,\n",
      "          2848,  2217,  9084,  1596,   119,  1252,  1103,  6047,  2656,  1104,\n",
      "          1234,  1150, 22058,  1142,  1132,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': '“Free Palestine” isn’t inherently antisemitic. But the vast majority of people who chant this are.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', '“', 'Free', 'Palestine', '”', 'isn', '’', 't', 'inherent', '##ly', 'anti', '##se', '##mit', '##ic', '.', 'But', 'the', 'vast', 'majority', 'of', 'people', 'who', 'chant', 'this', 'are', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109, 14380, 14365,  2102,  1468,  7193,  1106,  4384,  1110,\n",
      "          1753,  2809,  8329,  2217,  9084,  1596,   117,  1135,   787,   188,\n",
      "          6489,  4184,  5114, 15421,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}, 'orig_title': 'The Different Standards Westerners Give to Jews is Not Only Antisemitic, It’s Islamophobic', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'The', 'Different', 'Standards', 'Western', '##ers', 'Give', 'to', 'Jews', 'is', 'Not', 'Only', 'Anti', '##se', '##mit', '##ic', ',', 'It', '’', 's', 'Islam', '##op', '##ho', '##bic', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1110,  3103,  1136, 14854,  1537,  3085,  1105, 15109,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why is Israel not occupying West bank and Gaza?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'is', 'Israel', 'not', 'occupying', 'West', 'bank', 'and', 'Gaza', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3291, 11708,  1776,  7008,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Coexistence!', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Co', '##ex', '##ist', '##ence', '!', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7786, 15397,  1113,  3103,   112,   188,  3799, 24664,  6530,\n",
      "          7117,  1414, 18050,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Deep Intel on Israel's Post Ceasefire War Strategy\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Deep', 'Intel', 'on', 'Israel', \"'\", 's', 'Post', 'Ce', '##ase', '##fire', 'War', 'Strategy', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   157, 24887,  3237,  1116,  8988, 20111,  2181,   170, 10319,\n",
      "         20111,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Tweet Shows Palestinian Cause Is a Colonial Cause', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'T', '##weet', 'Show', '##s', 'Palestinian', 'Cause', 'Is', 'a', 'Colonial', 'Cause', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  8988,  1345,  5641,  1130, 22474,  5100,   117,  9723,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A Palestinian March protest In Valletta,Malta ', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'Palestinian', 'March', 'protest', 'In', 'Valle', '##tta', ',', 'Malta', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 23726,   145,  1183,  5674,  1665,  4889,  1183,  1105, 12848,\n",
      "         27604,  1116,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Zionist Hypocrisy and Turning Tides', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Zionist', 'H', '##y', '##po', '##c', '##ris', '##y', 'and', 'Turning', 'Tide', '##s', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 3100, 1195, 1518, 1267, 2243, 1746, 3519,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Will we ever see middle east peace?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Will', 'we', 'ever', 'see', 'middle', 'east', 'peace', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1646,  1105,  4878,  3878, 10573,  1106,  3510, 27338,  2592,\n",
      "          1115,  3103,  1450, 24574,  1108,  3693,  2035,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'US and Israeli officials react to blockbuster report that Israel knew Hamas was planning attack', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'US', 'and', 'Israeli', 'officials', 'react', 'to', 'block', '##buster', 'report', 'that', 'Israel', 'knew', 'Hamas', 'was', 'planning', 'attack', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  2183,  1142,  1594,  1156,  1129,  1618,  1111,  4554,\n",
      "          2050, 17054,  1116,  1190,  5074,  5871,  7941,  2215,  1107,  1540,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel winning this war would be better for palestinians than letting hamas stay in power', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'winning', 'this', 'war', 'would', 'be', 'better', 'for', 'pale', '##st', '##inian', '##s', 'than', 'letting', 'ha', '##mas', 'stay', 'in', 'power', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   118,  8619,   117,  1103,  2801,  1104,  1160,  2073,\n",
      "          2347,  1107,  1607,   112,   188, 19771,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Israel-Palestine, the stories of two families caught in history's turmoil\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', '-', 'Palestine', ',', 'the', 'stories', 'of', 'two', 'families', 'caught', 'in', 'history', \"'\", 's', 'turmoil', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1731, 1202, 1195, 1494,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How do we help?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'do', 'we', 'help', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  1106,  1138,  5017, 12705,  1114,  1103,  1168,  1334,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How to have calm conversations with the other side?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'to', 'have', 'calm', 'conversations', 'with', 'the', 'other', 'side', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 23994,  3309,  1403,  1306, 14104,  4964,  1107,  2710,  9126,\n",
      "          4729,  1320,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Paradigm Shift in Public Opinion', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Para', '##di', '##g', '##m', 'Shi', '##ft', 'in', 'Public', 'Op', '##ini', '##on', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 19312,  1116,  1105,  2582,  3213,  1107,  1103, 15109, 18534,\n",
      "          1121,  1360,  1572,  1106,  1379,  1572,   117, 17881,  1495,   113,\n",
      "          5989,  7746,  2233,  1104,  4138,  2473,  5902,  1757,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Bombs and plant growth in the Gaza Strip from August 24 to November 24, 2023 (satellite radar data of increasing surface roughness)', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Bomb', '##s', 'and', 'plant', 'growth', 'in', 'the', 'Gaza', 'Strip', 'from', 'August', '24', 'to', 'November', '24', ',', '202', '##3', '(', 'satellite', 'radar', 'data', 'of', 'increasing', 'surface', 'rough', '##ness', ')', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,   112,   188,  2673,  6969,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Hamas's sex crimes\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Hamas', \"'\", 's', 'sex', 'crimes', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1914,  118, 4520, 1106,  123,  118, 1426, 5072,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Road-map to 2-State solution', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Road', '-', 'map', 'to', '2', '-', 'State', 'solution', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2082, 4249, 1282, 1113, 4033,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Most dangerous place on earth.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Most', 'dangerous', 'place', 'on', 'earth', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101,  138, 4148, 1526, 1187, 1195, 1241, 2824, 1380, 1121, 1103,  107,\n",
      "         1168, 1334,  107,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A documentary club where we both watch something from the \"other side\".', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'A', 'documentary', 'club', 'where', 'we', 'both', 'watch', 'something', 'from', 'the', '\"', 'other', 'side', '\"', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 21863,  1863,   131,  1109,  7013, 16573,  3103, 18491,  1174,\n",
      "          1106, 19390,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Zionism: The Secret Evidence Israel Tried to Bury', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Zion', '##ism', ':', 'The', 'Secret', 'Evidence', 'Israel', 'Tri', '##ed', 'to', 'Bury', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2181, 3519, 1936,  136,  138, 5835,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Is peace possible? A proposal', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Is', 'peace', 'possible', '?', 'A', 'proposal', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2825, 1800, 1587, 1143, 1165, 8619, 1108, 1148, 1771,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Can someone tell me when Palestine was first founded?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Can', 'someone', 'tell', 'me', 'when', 'Palestine', 'was', 'first', 'founded', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 16742,  1805,  1104, 15109,   117,  1184,  1202,  1128,  1341,\n",
      "          1164,  1122,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Autonomous region of Gaza, what do you think about it?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Autonomous', 'region', 'of', 'Gaza', ',', 'what', 'do', 'you', 'think', 'about', 'it', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1258, 1103, 1594,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}, 'orig_title': 'After the war', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'After', 'the', 'war', '[SEP]']}, {'title': {'input_ids': tensor([[ 101,  145, 1183, 5674, 1665, 4889, 1183,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hypocrisy', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'H', '##y', '##po', '##c', '##ris', '##y', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  6381,  8635,   112,   188,  2705,  1104,  2546,  1867,  3103,\n",
      "          1209,  1136,  1129,   170,  1352,  1107,  1406,  1201,  1105,  1115,\n",
      "          1122,   112,   188,   107,  1103,  1211,  2620,  1352,  1107,  1103,\n",
      "          1362,  1106,  1321,  1103,  1646,  1106,  1981, 15841,  3842,   107,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Colin Powell\\'s chief of staff says Israel will not be a state in 20 years and that it\\'s \"the most likely state in the world to take the US to armageddon\"', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Colin', 'Powell', \"'\", 's', 'chief', 'of', 'staff', 'says', 'Israel', 'will', 'not', 'be', 'a', 'state', 'in', '20', 'years', 'and', 'that', 'it', \"'\", 's', '\"', 'the', 'most', 'likely', 'state', 'in', 'the', 'world', 'to', 'take', 'the', 'US', 'to', 'arm', '##aged', '##don', '\"', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4271, 12542, 12440,  8619,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Roger Waters Supporting Palestine', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Roger', 'Waters', 'Supporting', 'Palestine', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103, 15141,  9112,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}, 'orig_title': 'Israel targeting civilians', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'targeting', 'civilians', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 19439,  1120, 20285, 23755,  1178,  1774,  1106,  1243,  2094,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Shooting at starving Palestinians only trying to get food', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Shooting', 'at', 'starving', 'Palestinians', 'only', 'trying', 'to', 'get', 'food', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1731, 1110, 1103, 1690, 2820, 1107, 3103,  120,  157, 2162, 2559,\n",
      "         1268, 1208,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How is the living situation in Israel/TLV right now?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'is', 'the', 'living', 'situation', 'in', 'Israel', '/', 'T', '##L', '##V', 'right', 'now', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  1401, 18028,  1117, 10283,  1112,  1119, 12667,  1117,\n",
      "          1797,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A father expresses his grief as he loses his daughter ', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'A', 'father', 'expresses', 'his', 'grief', 'as', 'he', 'loses', 'his', 'daughter', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   789,  1622,  1103,  2186,  1106,  1103,  2343,   790,  1110,\n",
      "          2213,  1111, 23755,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '“From the river to the sea” is bad for Palestinians', 'label': -1, 'str_tokenized_sentence': ['[CLS]', '“', 'From', 'the', 'river', 'to', 'the', 'sea', '”', 'is', 'bad', 'for', 'Palestinians', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   158,   119,   156,   119,  6962,  3548,   112,  6189, 14552,\n",
      "           112,  5022,  1174,  4672,  1193,  1121,  1357,   128,  2035,   118,\n",
      "           146, 19598, 22680, 18019,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}, 'orig_title': \"U.S. researchers claim 'informed traders' profited massively from October 7 attack - I24NEWS\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'U', '.', 'S', '.', 'researchers', 'claim', \"'\", 'informed', 'traders', \"'\", 'profit', '##ed', 'massive', '##ly', 'from', 'October', '7', 'attack', '-', 'I', '##24', '##NE', '##WS', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2181, 1142, 1143, 3263, 8026,  136, 1327,  787,  188, 1240, 4893,\n",
      "         1104,  789, 4384, 1111, 8619,  790,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Is this meme accurate? What’s your opinion of “Jews for Palestine”?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Is', 'this', 'me', '##me', 'accurate', '?', 'What', '’', 's', 'your', 'opinion', 'of', '“', 'Jews', 'for', 'Palestine', '”', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4439, 21310,  1104,   112,  3021,  8421,   112,  1191,  3103,\n",
      "          7539, 24574,  6629,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Turkey warns of 'serious consequences' if Israel targets Hamas abroad\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Turkey', 'warns', 'of', \"'\", 'serious', 'consequences', \"'\", 'if', 'Israel', 'targets', 'Hamas', 'abroad', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 3103,  787,  188, 1594, 1222, 1482,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel’s war against children', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', '’', 's', 'war', 'against', 'children', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5140, 23660,  1158,  2830,  1106,  3244,  2778,  3911,   117,\n",
      "          1112,  2848,  2217,  9084,  1863,  9440,   119,  5313,   131,  1735,\n",
      "          2778,  2757,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Denmark deploying troops to protect Jewish sites, as antisemitism rises. Source: European Jewish Congress', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Denmark', 'deploy', '##ing', 'troops', 'to', 'protect', 'Jewish', 'sites', ',', 'as', 'anti', '##se', '##mit', '##ism', 'rises', '.', 'Source', ':', 'European', 'Jewish', 'Congress', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   140,  4359,  5225,  3242,  1111,  1234,  6604,  1107,  1103,\n",
      "          1244,  1311,  1106,  1494, 23755, 15278, 12240,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Credible ways for people residing in the United States to help Palestinians ASAP?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'C', '##red', '##ible', 'ways', 'for', 'people', 'residing', 'in', 'the', 'United', 'States', 'to', 'help', 'Palestinians', 'AS', '##AP', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 19312,  1158,  1104, 15109,  1144,  4938,  1137,  3072,  1167,\n",
      "          1190,  1620,  5900,  3911,   117, 23170,  2592,  7189,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Bombing of Gaza has damaged or destroyed more than 100 heritage sites, NGO report reveals', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Bomb', '##ing', 'of', 'Gaza', 'has', 'damaged', 'or', 'destroyed', 'more', 'than', '100', 'heritage', 'sites', ',', 'NGO', 'report', 'reveals', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 10605,  6871,  9723,   113,  1134,  1110,   170, 20093,  2371,\n",
      "          2394,   114,  1110,  2140,  4000,  1103,  3958,  1104,  1184,   146,\n",
      "          2346,  2271,  1110,  1833,  1107, 15109,  1105,  1122,  2196,  1103,\n",
      "          3062,  1106, 20093,  4037,   113,  1136,  1198,  1106, 20093,  1133,\n",
      "          1145,  1106, 16714,  1315,  1150,  2077,  1107,  9723,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, 'orig_title': 'Lovin Malta (which is a Maltese news media) is actually showing the reality of what IOF is doing in Gaza and it shows the truth to Maltese citizens (not just to Maltese but also to foreigners too who lived in Malta)', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Lo', '##vin', 'Malta', '(', 'which', 'is', 'a', 'Maltese', 'news', 'media', ')', 'is', 'actually', 'showing', 'the', 'reality', 'of', 'what', 'I', '##O', '##F', 'is', 'doing', 'in', 'Gaza', 'and', 'it', 'shows', 'the', 'truth', 'to', 'Maltese', 'citizens', '(', 'not', 'just', 'to', 'Maltese', 'but', 'also', 'to', 'foreigners', 'too', 'who', 'lived', 'in', 'Malta', ')', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  3791,  1167, 15109,  2316,  1106, 10556,   117, 10095,\n",
      "          1877,  1187,  1122, 10130,  1172,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel orders more Gazans to flee, bombs areas where it sends them', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'orders', 'more', 'Gaza', '##ns', 'to', 'flee', ',', 'bombs', 'areas', 'where', 'it', 'sends', 'them', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 16409,  5096,   118,  8619,  8704,  1468,   117,  1184,  2014,\n",
      "          1240,  1713,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Ex Pro-Palestine Supporters, what changed your mind?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Ex', 'Pro', '-', 'Palestine', 'Support', '##ers', ',', 'what', 'changed', 'your', 'mind', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 11600,  3121,  1306,  1104,  8329,   118,  8988, 16402, 19439,\n",
      "           113,  1230,  2522,   138, 18320,  7192,   114,  8123, 23994,  1193,\n",
      "          5305,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Victim of Anti-Palestinian Burlington Shooting (Hisham Awartani) Left Paralyzed', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Vic', '##ti', '##m', 'of', 'Anti', '-', 'Palestinian', 'Burlington', 'Shooting', '(', 'His', '##ham', 'A', '##wart', '##ani', ')', 'Left', 'Para', '##ly', '##zed', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 19016, 10147,  1658,  1108,  1621,  1103,  1499,  1406,  4511,\n",
      "          1468,  1107,  1103, 17881,  1477,  3212,   119,  3446,   787,   188,\n",
      "          1293,  1122,  7610,  1205,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, 'orig_title': 'AIPAC was among the top 20 spenders in the 2022 elections. Here’s how it breaks down.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'AI', '##PA', '##C', 'was', 'among', 'the', 'top', '20', 'spend', '##ers', 'in', 'the', '202', '##2', 'elections', '.', 'Here', '’', 's', 'how', 'it', 'breaks', 'down', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  2803,  1132,  5569,  1103, 20852,  1104,  1103,  1482,\n",
      "          1152, 20812,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel soldiers are riding the bikes of the children they bombed .', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'soldiers', 'are', 'riding', 'the', 'bikes', 'of', 'the', 'children', 'they', 'bombed', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1646, 20335,  1116,  1106,   789,  2760,  1106,  1619,  3103,\n",
      "           790,  1112,  1166,   122,   117,  1288,  1167, 15109,  2316,  1132,\n",
      "         11584,  1181,  1166,  1103,  5138,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, 'orig_title': 'US pledges to “continue to support Israel” as over 1,000 more Gazans are massacred over the weekend', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'US', 'pledge', '##s', 'to', '“', 'continue', 'to', 'support', 'Israel', '”', 'as', 'over', '1', ',', '000', 'more', 'Gaza', '##ns', 'are', 'massacre', '##d', 'over', 'the', 'weekend', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 12596,  1103,  8988,  1482,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Save the Palestinian children!', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Save', 'the', 'Palestinian', 'children', '!', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2778,  1651, 13927,  1106,  2848,  2217,  9084,  1596,  6704,\n",
      "          1120,  1498,  2755,   118,  1735,  2778,  2757,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Jewish students subjected to antisemitic abuse at London university - European Jewish Congress', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Jewish', 'students', 'subjected', 'to', 'anti', '##se', '##mit', '##ic', 'abuse', 'at', 'London', 'university', '-', 'European', 'Jewish', 'Congress', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1752, 24574,  7705, 16567,  1123,   119,  1599,  1152,  2046,\n",
      "          1123,  1107,  1103,  1246,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'First Hamas fighters raped her. Then they shot her in the head', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'First', 'Hamas', 'fighters', 'raped', 'her', '.', 'Then', 'they', 'shot', 'her', 'in', 'the', 'head', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   118,  8619, 23419,   131,   122,   118,   117,   123,\n",
      "           118,  1137,   124,   118,  1426,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel-Palestine Conflict: 1-,2- or 3-State?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', '-', 'Palestine', 'Conflict', ':', '1', '-', ',', '2', '-', 'or', '3', '-', 'State', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1212, 13359,  1182,   119,  1382,   129,   117,  1195,  1209,\n",
      "           108, 15104,  2240,  1204,  2137, 13798,  1527,  2101, 19856, 12569,\n",
      "          1517,  1254,   119,  1247,  1169,  1129,  1185,  1671,  1112,  4400,\n",
      "          1229, 19643,  1110,  1781,  1282,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'On Fri. December 8, we will #ShutItDown4Palestine once again. There can be no business as usual while genocide is taking place!', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'On', 'Fr', '##i', '.', 'December', '8', ',', 'we', 'will', '#', 'Shut', '##I', '##t', '##D', '##own', '##4', '##P', '##ales', '##tine', 'once', 'again', '.', 'There', 'can', 'be', 'no', 'business', 'as', 'usual', 'while', 'genocide', 'is', 'taking', 'place', '!', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5051,  1651,   786, 10444,   787,  1170,  5250,   118,  8988,\n",
      "         14339, 26499,  3553,  1114,  2848,  2217,  9084,  1596, 22058,  1116,\n",
      "          1105, 12200,  9772,  1116,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}, 'orig_title': 'Harvard students ‘terrified’ after pro-Palestinian protesters disrupt classes with antisemitic chants and bullhorns', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Harvard', 'students', '‘', 'terrified', '’', 'after', 'pro', '-', 'Palestinian', 'protesters', 'disrupt', 'classes', 'with', 'anti', '##se', '##mit', '##ic', 'chant', '##s', 'and', 'bull', '##horn', '##s', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  1225, 24574,  1138,  1106,  4361,  1121,  1103, 14551,\n",
      "          3670,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What did Hamas have to gain from the hostage exchange?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'did', 'Hamas', 'have', 'to', 'gain', 'from', 'the', 'hostage', 'exchange', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 22171,   131,  2009,  2144,   112,   189,  3103,  1383,  1146,\n",
      "           170, 26707, 18575,  6817,  2200,  4834,  1113,  1103,  4878,  1334,\n",
      "          1104,  1103,  3070,  1114, 15109,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': \"Question: Why doesn't Israel set up a demilitarized zone on the Israeli side of the border with Gaza?\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Question', ':', 'Why', 'doesn', \"'\", 't', 'Israel', 'set', 'up', 'a', 'dem', '##ili', '##tar', '##ized', 'zone', 'on', 'the', 'Israeli', 'side', 'of', 'the', 'border', 'with', 'Gaza', '?', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 7414, 1535,  112,  188, 4449, 1104, 1103, 5772, 1359, 4289, 1113,\n",
      "         1357, 4766,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"UN women's investigation of the gender based violence on October 7th\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'UN', 'women', \"'\", 's', 'investigation', 'of', 'the', 'gender', 'based', 'violence', 'on', 'October', '7th', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1960,  118, 1352, 5072,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Two-state solution', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Two', '-', 'state', 'solution', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   107,  4012, 18418,   107,  1169,  1294,  1128,  3513, 10302,\n",
      "          1234,  8564,  1105, 16281,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '\"Military fatigue\" can make you murder disabled people ladies and gentlemen', 'label': 0, 'str_tokenized_sentence': ['[CLS]', '\"', 'Military', 'fatigue', '\"', 'can', 'make', 'you', 'murder', 'disabled', 'people', 'ladies', 'and', 'gentlemen', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5157,  1116,  1104,  4674,  1104, 23755,  3359,  1106, 16890,\n",
      "          8057,  1324,  1331,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Tens of thousands of Palestinians escape to Rafah city', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Ten', '##s', 'of', 'thousands', 'of', 'Palestinians', 'escape', 'to', 'Ra', '##fa', '##h', 'city', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1249,   170, 17706,   146,  1821,  2959,   119,   146,  1821,\n",
      "          1177,  2959,   119,  4299,  8619,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'As a Jew I am sorry. I am so sorry. Free Palestine.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'As', 'a', 'Jew', 'I', 'am', 'sorry', '.', 'I', 'am', 'so', 'sorry', '.', 'Free', 'Palestine', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1249, 2848, 2217, 9084, 1863, 1177, 7666,  117, 1111, 1242, 4384,\n",
      "         1107, 1699, 8251, 5115, 2914,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'As antisemitism soars, for many Jews in France nowhere feels safe', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'As', 'anti', '##se', '##mit', '##ism', 'so', '##ars', ',', 'for', 'many', 'Jews', 'in', 'France', 'nowhere', 'feels', 'safe', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 12118, 22116,  1979,  1103,   143,  6639, 15353,  1906,  4878,\n",
      "          9059,  1104,  5096,  4163,  3820,  1810,   131,   138, 15247,  4785,\n",
      "          1120,  1398, 24097,  1116,  1104,  8718, 21843,   138,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}, 'orig_title': 'Unraveling the Fabricated Israeli Web of Propaganda: A Critical Look at Allegations of Mass Sexual A', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Un', '##rave', '##ling', 'the', 'F', '##ab', '##rica', '##ted', 'Israeli', 'Web', 'of', 'Pro', '##pa', '##gan', '##da', ':', 'A', 'Critical', 'Look', 'at', 'All', '##egation', '##s', 'of', 'Mass', 'Sexual', 'A', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  2803,  5601,  1121, 12384, 19138, 18882,  7867,  2645,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli soldiers suffering from Gastronomical problems ', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'soldiers', 'suffering', 'from', 'Gas', '##tron', '##omi', '##cal', 'problems', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  1110,  1606, 19016,  1106, 20446,  8602,  7539,  1107,\n",
      "         15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel is using AI to calculate bombing targets in Gaza', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'is', 'using', 'AI', 'to', 'calculate', 'bombing', 'targets', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 14123,  1104,  2270, 22961,  8362,  8009, 16719,  4453,  1104,\n",
      "          1352,  1104,  8619,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'PM of Ireland rejects unilateral recognition of state of Palestine', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'PM', 'of', 'Ireland', 'rejects', 'un', '##ila', '##teral', 'recognition', 'of', 'state', 'of', 'Palestine', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138, 14566, 17430,  1121, 14551,  2309,  1114,  1103,  4878,\n",
      "          1414,  9049,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Accounts from hostage meeting with the Israeli War Cabinet', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'A', '##cco', '##unts', 'from', 'hostage', 'meeting', 'with', 'the', 'Israeli', 'War', 'Cabinet', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  1285,  1104,  5915,  1107,  1103,  1537,  2950,   117,\n",
      "          1695,  1379, 17881,  1475,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A day of destruction in the West Bank, 23 November 2021', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'day', 'of', 'destruction', 'in', 'the', 'West', 'Bank', ',', '23', 'November', '202', '##1', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1130, 14638,  1103,  9198, 13335,  3269,  5818, 24819,  2924,\n",
      "           106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Invoke the Genocide Convention NOW!', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'In', '##voke', 'the', 'Gen', '##oc', '##ide', 'Convention', 'NO', '##W', '!', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 8619, 3100, 4108, 4299,  118, 3765,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestine Will Be Free - Song', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestine', 'Will', 'Be', 'Free', '-', 'Song', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  1169,   787,   189,  3326, 24574,  1107,  2321,   117,\n",
      "          1177,  1184,   787,   188,  1397,   136,   197,  1398,  1103,  4139,\n",
      "          1107, 15109,  1110, 11190,  1110,  6688, 17490,   117,  1105,  1103,\n",
      "          1646,  1169,  1831,  1122,  1120,  1251,  1159,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel can’t defeat Hamas in battle, so what’s next? | All the conflict in Gaza is achieving is civilian misery, and the US can stop it at any time', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'can', '’', 't', 'defeat', 'Hamas', 'in', 'battle', ',', 'so', 'what', '’', 's', 'next', '?', '|', 'All', 'the', 'conflict', 'in', 'Gaza', 'is', 'achieving', 'is', 'civilian', 'misery', ',', 'and', 'the', 'US', 'can', 'stop', 'it', 'at', 'any', 'time', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  1138,  1151,  1606,  1769, 18254,  1120,  1655,  1112,\n",
      "          1677,  1171,  1112,  1387,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas have been using human shields at least as far back as 2014', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'have', 'been', 'using', 'human', 'shields', 'at', 'least', 'as', 'far', 'back', 'as', '2014', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7414,  2705,  1107, 14638,  1116,  3342,  4850,  1113, 15109,\n",
      "          1107,  4054,   117,  3110,  1815,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'UN chief invokes article 99 on Gaza in rare, powerful move', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'UN', 'chief', 'in', '##voke', '##s', 'article', '99', 'on', 'Gaza', 'in', 'rare', ',', 'powerful', 'move', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1130,  1103,  1257,  1104,  1103,  1470,  2175,   111,  4265,\n",
      "          1934,  2394,   117,  1293,  3903,  1110,  1103,  1769,  7292, 14417,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'In the eyes of the public court & global social media, how effective is the human shield reasoning?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'In', 'the', 'eyes', 'of', 'the', 'public', 'court', '&', 'global', 'social', 'media', ',', 'how', 'effective', 'is', 'the', 'human', 'shield', 'reasoning', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   144, 18614,  4894, 27169,  4354,  1761,  1106,  2496,  1166,\n",
      "          3103,   118,  8619,  5532,  1112,  4433,  1106,  1835,  3519,  1105,\n",
      "          2699,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Guterres urges Security Council to act over Israel-Palestine crisis as threat to international peace and security', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'G', '##uter', '##res', 'urges', 'Security', 'Council', 'to', 'act', 'over', 'Israel', '-', 'Palestine', 'crisis', 'as', 'threat', 'to', 'international', 'peace', 'and', 'security', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24991,  8908,  1104, 24574, 24428,  1105,  4252,   118, 21832,\n",
      "         15034, 20820, 18266, 23222,   197,  3103,   118, 24574,  1594,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Angry relatives of Hamas captives and ex-hostages confront Netanyahu | Israel-Hamas war', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Angry', 'relatives', 'of', 'Hamas', 'captives', 'and', 'ex', '-', 'hostages', 'confront', 'Net', '##any', '##ahu', '|', 'Israel', '-', 'Hamas', 'war', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   786,   138,  3367, 10395,  4790,   787,   131,  7323,  3103,\n",
      "           787,   188, 10056,  8602,  1104, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '‘A mass assassination factory’: Inside Israel’s calculated bombing of Gaza', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '‘', 'A', 'mass', 'assassination', 'factory', '’', ':', 'Inside', 'Israel', '’', 's', 'calculated', 'bombing', 'of', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 11930, 21832,  3522,  1106, 20820,  1424, 18713,  1358,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Released hostages speaking to Netenyahu', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Released', 'hostages', 'speaking', 'to', 'Net', '##en', '##yah', '##u', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4421,  5146,  3093,  1167,  8394,  1190,   170,   123, 12480,\n",
      "          1120,  1142,  1553,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Jordan option seems more attractive than a 2SS at this point', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Jordan', 'option', 'seems', 'more', 'attractive', 'than', 'a', '2', '##SS', 'at', 'this', 'point', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7277,  2386,  3103,   118, 24574,  1594,   117,  1651,  1474,\n",
      "          2848,  2217,  9084,  1863,  1110,   112,  1207,  2999,   112,  1120,\n",
      "          3132,  1239,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Amid Israel-Hamas war, students say antisemitism is 'new normal' at Columbia University\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Am', '##id', 'Israel', '-', 'Hamas', 'war', ',', 'students', 'say', 'anti', '##se', '##mit', '##ism', 'is', \"'\", 'new', 'normal', \"'\", 'at', 'Columbia', 'University', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 12958,  7831,  6056,  1104,  9511,  2309,  7189, 21832,   787,\n",
      "         10842,  1120, 20820, 18266, 23222,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Leaked audio of heated meeting reveals hostages’ fury at Netanyahu', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Lea', '##ked', 'audio', 'of', 'heated', 'meeting', 'reveals', 'hostages', '’', 'fury', 'at', 'Net', '##any', '##ahu', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878, 14123,  5816, 20820, 18266, 23222,  1130, 22398,  1116,\n",
      "          1115,  3103,  1431,  1138,  2699,  1654,  1166,  1103, 15109, 18534,\n",
      "          1170,  1103,  1594,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli PM Benjamin Netanyahu Insists that Israel should have security control over the Gaza Strip after the war', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'PM', 'Benjamin', 'Net', '##any', '##ahu', 'In', '##sist', '##s', 'that', 'Israel', 'should', 'have', 'security', 'control', 'over', 'the', 'Gaza', 'Strip', 'after', 'the', 'war', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1790,   787,   189,  2421,  2102, 14329,   146, 25566, 10555,\n",
      "         14177,  5026, 17030,  3171,  1192,  1706,  1109,   157, 20240,  3810,\n",
      "          1107, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Don’t Let Western Collegiate Ignorance Desensitize You To The Tragedy in Gaza', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Don', '’', 't', 'Let', 'Western', 'Collegiate', 'I', '##gno', '##rance', 'Des', '##ens', '##iti', '##ze', 'You', 'To', 'The', 'T', '##rage', '##dy', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 11050,  1106,  3103,  1106,  8676,  1105,  1112,  1122,  3370,\n",
      "          2739,   146,   787,   182,  1167,  1105,  1167,  5604,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Coming to Israel to volunteer and as it gets closer I’m more and more nervous', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Coming', 'to', 'Israel', 'to', 'volunteer', 'and', 'as', 'it', 'gets', 'closer', 'I', '’', 'm', 'more', 'and', 'more', 'nervous', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5096,   118,  8938, 12775,  1107,  1103,  2102, 10709,  2230,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Pro-terror sympathy in the Western Progressive movement.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Pro', '-', 'terror', 'sympathy', 'in', 'the', 'Western', 'Progressive', 'movement', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1302,  2187,  1184,  1110,  1240,  2458,   118,  1109,  4583,\n",
      "         19398,  1110,  1103,  4554,  2050, 17054,  2612,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'No matter what is your view - The biggest loser is the palestinian cause', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'No', 'matter', 'what', 'is', 'your', 'view', '-', 'The', 'biggest', 'loser', 'is', 'the', 'pale', '##st', '##inian', 'cause', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2778,  5096, 13053,  1468,   159, 19840, 23680, 11938,  1424,\n",
      "          1650,  4878,  3284,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Jewish Protesters Violently Beaten By Israeli Police', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Jewish', 'Pro', '##test', '##ers', 'V', '##iol', '##ently', 'Beat', '##en', 'By', 'Israeli', 'Police', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   789,  4243,  2266,   790,  2369,  4264,  1164,   789, 14940,\n",
      "         26107,   790,  1104, 24574, 17219,  1113,  1357,   128,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '“Human rights” organization concerned about “summary executions” of Hamas terrorists on October 7.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', '“', 'Human', 'rights', '”', 'organization', 'concerned', 'about', '“', 'summary', 'executions', '”', 'of', 'Hamas', 'terrorists', 'on', 'October', '7', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  4878,  1116,  3374,  1573,   142, 20158,  1556,   152,\n",
      "         19515, 26392,   118, 12310, 16809,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How Israelis Live So Easily With Occupation - Gideon Levy', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'How', 'Israeli', '##s', 'Live', 'So', 'E', '##asily', 'With', 'O', '##cc', '##upation', '-', 'Gideon', 'Levy', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1479,   117, 27672,  1234,  6636,  1107, 15109,   119,   128,\n",
      "           117, 11150,  1104,  1172,  1482,   119, 20820, 18266, 23222,  1110,\n",
      "          1103,   108, 25991,  2346,  2087,  2349, 10961,  1161,   117,  1105,\n",
      "          1119,   112,  1325,  2283,  5299,  1107,   170,  7396,   112,   188,\n",
      "         11020,  1107,  1103, 15201,   117,  1198,  1176, 22644,  2217, 15901,\n",
      "           119,   156,  1643, 11613,  1142,  1911,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"16,248 people murdered in Gaza. 7,112 of them children. Netanyahu is the #ButcherOfGaza, and he'll meet justice in a prisoner's dock in the Hague, just like Milosevic. Spread this idea!\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', '16', ',', '248', 'people', 'murdered', 'in', 'Gaza', '.', '7', ',', '112', 'of', 'them', 'children', '.', 'Net', '##any', '##ahu', 'is', 'the', '#', 'Butcher', '##O', '##f', '##G', '##az', '##a', ',', 'and', 'he', \"'\", 'll', 'meet', 'justice', 'in', 'a', 'prisoner', \"'\", 's', 'dock', 'in', 'the', 'Hague', ',', 'just', 'like', 'Milo', '##se', '##vic', '.', 'S', '##p', '##read', 'this', 'idea', '!', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,   107, 14780, 26300, 21832,   107,   131,  1195,  1127,\n",
      "         18049,  3737,  1115,  1136, 24574,  1133,  3103,  1156,  2311,  1366,\n",
      "           117,  1105,  1173,  1152,  1156,  1474,   118, 24574,  1841,  1128,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The \"traumatized hostages\": we were terribly afraid that not Hamas but Israel would kill us, and then they would say - Hamas killed you.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', '\"', 'trauma', '##tized', 'hostages', '\"', ':', 'we', 'were', 'terribly', 'afraid', 'that', 'not', 'Hamas', 'but', 'Israel', 'would', 'kill', 'us', ',', 'and', 'then', 'they', 'would', 'say', '-', 'Hamas', 'killed', 'you', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,   789,  8619,   790, 19440, 17721,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The “Palestine” Smokescreen', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'The', '“', 'Palestine', '”', 'Smoke', '##screen', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1760, 27140,  5176, 18381,  1107, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'An IDF soldier proposes in Gaza', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'An', 'IDF', 'soldier', 'proposes', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1697,  1104, 19753,  1424,  1179,  3301,  2757,   789,  3516,\n",
      "          1111,  1103, 19643,  1104,   179, 17540,   790,  1178, 26586,  1116,\n",
      "          2755,   787,   188,  2995,  1191,   789,  1103,  4055,  3587,  1154,\n",
      "          5880,   790,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'President of UPenn tells Congress “calling for the genocide of jews” only violates university’s rules if “the speech turns into conduct”', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'President', 'of', 'UP', '##en', '##n', 'tells', 'Congress', '“', 'calling', 'for', 'the', 'genocide', 'of', 'j', '##ews', '”', 'only', 'violate', '##s', 'university', '’', 's', 'rules', 'if', '“', 'the', 'speech', 'turns', 'into', 'conduct', '”', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574, 18686,   117,  1111,  1103,  5250,   118, 24574,  6638,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas motives, for the pro-Hamas supporters', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'motives', ',', 'for', 'the', 'pro', '-', 'Hamas', 'supporters', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,  3711,  1122,  1110,  1136,  5237,  2716,  4044,  4253,\n",
      "         15109,   119,  2421,   112,   188,  1474,  1111,  6171,   112,   188,\n",
      "          8590,   117,  1191,   170,  1583,  1127,  1106, 16621,  1126,  5237,\n",
      "          4044,  4253,   117,  1156,  1122,  1440,  1251,  1472,  1190,  1184,\n",
      "          1195,  1267,  1208,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Israel claims it is not ethnically cleansing Gaza. Let's say for argument's sake, if a country were to execute an ethnic cleansing, would it look any different than what we see now?\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'claims', 'it', 'is', 'not', 'ethnic', '##ally', 'clean', '##sing', 'Gaza', '.', 'Let', \"'\", 's', 'say', 'for', 'argument', \"'\", 's', 'sake', ',', 'if', 'a', 'country', 'were', 'to', 'execute', 'an', 'ethnic', 'clean', '##sing', ',', 'would', 'it', 'look', 'any', 'different', 'than', 'what', 'we', 'see', 'now', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,   112,  1762, 20132,   112,  8988,  2351, 18975,  1117,\n",
      "          3289,  1401,   787,   188,  3513,   117,   170,  1607,  1104,  1835,\n",
      "         16793,  1105,  1260, 11509,  7984,  1107, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}, 'orig_title': \"A 'heartbroken' Palestinian author confronts his prominent father’s murder, a history of international betrayal and devastation in Gaza\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', \"'\", 'heart', '##broken', \"'\", 'Palestinian', 'author', 'confronts', 'his', 'prominent', 'father', '’', 's', 'murder', ',', 'a', 'history', 'of', 'international', 'betrayal', 'and', 'de', '##vas', '##tation', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1237,  4391,  9234,  2405,  7155,  1725,  3103,  1110, 15141,\n",
      "          9891,  1107, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'American journalist Abby Martin explains why Israel is targeting journalists in Gaza', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'American', 'journalist', 'Abby', 'Martin', 'explains', 'why', 'Israel', 'is', 'targeting', 'journalists', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   156, 25141,  1110, 16109,  4645,  1568,  8988, 14870,  1113,\n",
      "          1103,  1278, 12313,  1120,  1139,  2755,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'SJP is planting 8000 Palestinian flags on the school lawn at my university', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'S', '##JP', 'is', 'planting', '800', '##0', 'Palestinian', 'flags', 'on', 'the', 'school', 'lawn', 'at', 'my', 'university', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,   177,  1183,  5674,  1665,  4889,  1183,  1104,  1103,\n",
      "          1177,   118,  1270,  5250,   118,  8988,  3515,  1107,  2102,   170,\n",
      "         18161,  1110,  2894, 20844,  5815,  4179,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': 'The hypocrisy of the so-called pro-Palestinian crowd in Western academia is beyond hilarious', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'The', 'h', '##y', '##po', '##c', '##ris', '##y', 'of', 'the', 'so', '-', 'called', 'pro', '-', 'Palestinian', 'crowd', 'in', 'Western', 'a', '##cademia', 'is', 'beyond', 'hi', '##lar', '##ious', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15109,  1473, 10484,  2849,  1132,  2412, 10682,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Gaza death toll numbers are generally reliable', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Gaza', 'death', 'toll', 'numbers', 'are', 'generally', 'reliable', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   144,  1348,   144,  9359,  1204, 15606,  1116,  1835,  3747,\n",
      "          1166,  7012,  1104,  1535,  5256,  1113, 14125,   119,   128,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Gal Gadot slams international silence over fate of women victims on Oct. 7', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'G', '##al', 'G', '##ado', '##t', 'slam', '##s', 'international', 'silence', 'over', 'fate', 'of', 'women', 'victims', 'on', 'Oct', '.', '7', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  2156, 17903,  1110,  2709,  1114,  3103,  6638,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why Reddit is filled with Israel supporters', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'Red', '##dit', 'is', 'filled', 'with', 'Israel', 'supporters', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138, 16203,  1370,  8619,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A Prayer For Palestine', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'Prayer', 'For', 'Palestine', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1674,  1103, 27140,  1105,  4878,  1433,  2621, 21010,\n",
      "          2803,  1105,  7056,  2496,  1114, 24034, 22534,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why does the IDF and Israeli government allow rogue soldiers and settlers act with impunity?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Why', 'does', 'the', 'IDF', 'and', 'Israeli', 'government', 'allow', 'rogue', 'soldiers', 'and', 'settlers', 'act', 'with', 'imp', '##unity', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7414,  8858,  1704,  4140,   144, 18614,  4894,  1107, 14638,\n",
      "          1116,  1103,  2439,  8311,  6806,  1120,  1117, 14364,  8554,  4850,\n",
      "           117,  1134,  2231,  1115,   112,  1103,  2909,   118,  1615,  1336,\n",
      "          2498,  1106,  1103,  2209,  1104,  1103,  4354,  1761,  1251,  2187,\n",
      "          1134,  1107,  1117,  4893,  1336, 16757,  1103,  5972,  1104,  1835,\n",
      "          3519,  1105,  2699,   119,   107,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'UN secrets general Anthony Guterres invokes the highest diplomatic tool at his disposal Article 99, which states that \\'the Secretary-General may bring to the attention of the Security Council any matter which in his opinion may threaten the maintenance of international peace and security.\" ', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'UN', 'secrets', 'general', 'Anthony', 'G', '##uter', '##res', 'in', '##voke', '##s', 'the', 'highest', 'diplomatic', 'tool', 'at', 'his', 'disposal', 'Article', '99', ',', 'which', 'states', 'that', \"'\", 'the', 'Secretary', '-', 'General', 'may', 'bring', 'to', 'the', 'attention', 'of', 'the', 'Security', 'Council', 'any', 'matter', 'which', 'in', 'his', 'opinion', 'may', 'threaten', 'the', 'maintenance', 'of', 'international', 'peace', 'and', 'security', '.', '\"', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8988,  4391,  7085, 18208, 13431,   154,  3556,  2572,  2328,\n",
      "         21540,  1117,  4067,  1773,  1219,  2302,  8602,  1118,  3103,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinian journalist Mahmoud Qaddoha captures his kids playing  during heavy bombing by Israel.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinian', 'journalist', 'Ma', '##hm', '##oud', 'Q', '##ad', '##do', '##ha', 'captures', 'his', 'kids', 'playing', 'during', 'heavy', 'bombing', 'by', 'Israel', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1188,  1110,   170,  1414,  1113,  4288,   117,  1105,   107,\n",
      "         19770,  6402,  1116,   107,  1132,  4735,   157, 14543,  1116,   131,\n",
      "          7414,  9741, 14663,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'This is a War on Children, and \"Safe Zones\" are Death Traps: UNICEF', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'This', 'is', 'a', 'War', 'on', 'Children', ',', 'and', '\"', 'Safe', 'Zone', '##s', '\"', 'are', 'Death', 'T', '##rap', '##s', ':', 'UN', '##IC', '##EF', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  2088, 16858, 24574,  2301, 14680, 20622, 14009,  7200,\n",
      "           112,   188,  2620,  4750,  3554,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Israeli forces surround Hamas leader Yahya Sinwar's likely hideout\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'forces', 'surround', 'Hamas', 'leader', 'Ya', '##hya', 'Sin', '##war', \"'\", 's', 'likely', 'hide', '##out', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1731,  3103,  1110, 16863,   122,   119,   129,  1550, 23755,\n",
      "          1154,  1126,  3871,   118,  6956,  1298,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'How Israel is squeezing 1.8 million Palestinians into an airport-sized area', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'How', 'Israel', 'is', 'squeezing', '1', '.', '8', 'million', 'Palestinians', 'into', 'an', 'airport', '-', 'sized', 'area', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2091,  5250,  4878,   185, 17059,  1116,  2824,  1103,  2140,\n",
      "         22419,  1105,  1610, 17224,  1104,   176, 10961,  1161,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Do pro Israeli peeps watch the actually bombings and carnage of gaza?', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Do', 'pro', 'Israeli', 'p', '##eep', '##s', 'watch', 'the', 'actually', 'bombings', 'and', 'car', '##nage', 'of', 'g', '##az', '##a', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8554,  4850,  1107, 18891,  1118,  7414,  2534,   118,  6524,\n",
      "          2699,  3193,  1106,  2283,  3516,  1111,  5670, 14092,  7117,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Article 99 invoked by UN Chief - forcing security council to meet calling for immediate ceasefire.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Article', '99', 'in', '##voked', 'by', 'UN', 'Chief', '-', 'forcing', 'security', 'council', 'to', 'meet', 'calling', 'for', 'immediate', 'cease', '##fire', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  4878,  2306, 22586,   170,  2301,  1107,  1412,  1661,\n",
      "           119,  1284,  1444,  1240,  1494,  1106,  2498,  1140,  1313,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The Israeli army abducted a leader in our community. We need your help to bring him home.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'The', 'Israeli', 'army', 'abducted', 'a', 'leader', 'in', 'our', 'community', '.', 'We', 'need', 'your', 'help', 'to', 'bring', 'him', 'home', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 11336, 26304,  1104,  4878, 24428, 26064,   786,   124, 21832,\n",
      "          1841,  1118,  4878,  1783,   117,   787,  9232,  1116,  1764,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Relative of Israeli captives confirms ‘3 hostages killed by Israeli fire,’ blasts military', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Re', '##lative', 'of', 'Israeli', 'captives', 'confirms', '‘', '3', 'hostages', 'killed', 'by', 'Israeli', 'fire', ',', '’', 'blast', '##s', 'military', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009, 15109,  1110,  1136,  4478,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why Gaza is not Singapore', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'Gaza', 'is', 'not', 'Singapore', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 16932, 12195, 16752,  2007,  1306,  2316,   112,   146,  6262,\n",
      "         17536,   112,  4878, 13704,  1116,  1107, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Bernie Sanders Condemns 'Immoral' Israeli Attacks in Gaza\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Bernie', 'Sanders', 'Con', '##de', '##m', '##ns', \"'\", 'I', '##mm', '##oral', \"'\", 'Israeli', 'Attack', '##s', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 22171,  1164,  1103,  1537,  2950,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Question about the West Bank', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Question', 'about', 'the', 'West', 'Bank', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8329,   118, 21863,  1863,  1105,  2848,   118, 14306, 15813,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Anti-Zionism and anti-Semitism', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Anti', '-', 'Zion', '##ism', 'and', 'anti', '-', 'Semi', '##tism', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2825,  1195,  3191,  1103, 12849,  1104,  1103, 27140,  1217,\n",
      "           107,  1103,  1211,  7279,  2306,   107,  1106,  1832,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Can we lay the myth of the IDF being\" the most moral army\" to rest', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Can', 'we', 'lay', 'the', 'myth', 'of', 'the', 'IDF', 'being', '\"', 'the', 'most', 'moral', 'army', '\"', 'to', 'rest', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,  1221,  1103,  3009,  7467,  1133,  1176,  1577,   112,\n",
      "           189,  2778,  1234,  1138,  1687,   170, 14764,  1107,   170,  1282,\n",
      "          1136,  4405,  1118,  1892, 14298,  9731,  1183,  3258,  4553,  1468,\n",
      "          1150,  1274,   112,   189,  1176,  1172,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"I know the historical significance but like couldn't Jewish people have created a homeland in a place not surrounded by bloodthirsty warmongers who don't like them.\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'I', 'know', 'the', 'historical', 'significance', 'but', 'like', 'couldn', \"'\", 't', 'Jewish', 'people', 'have', 'created', 'a', 'homeland', 'in', 'a', 'place', 'not', 'surrounded', 'by', 'blood', '##thi', '##rst', '##y', 'warm', '##ong', '##ers', 'who', 'don', \"'\", 't', 'like', 'them', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   786,  2586, 20350,  1158,   787,   131, 23755,   170, 19515,\n",
      "          5613, 17502, 16810,  1104, 15069,  1170,  3103,  3143,   197,  1507,\n",
      "          1117,  1210,   118,  1285,  3143,   117,  3103,  1145,  1225,  1136,\n",
      "          2621,  4340,  1106,  3873, 15109,   117,  1187,  3103,  1144,  1841,\n",
      "          1167,  1190,  1542,   117,  1288,  1234,  1105, 13577,  1211,  1104,\n",
      "          1103, 19571,  4035, 16768,   787,   188,   123,   119,   124,  1550,\n",
      "          4131,  1121,  1147,  4481,  1290,  1357,   128,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '‘Alarming’: Palestinians accuse ICC prosecutor of bias after Israel visit | During his three-day visit, Israel also did not allow Khan to enter Gaza, where Israel has killed more than 17,000 people and displaced most of the besieged enclave’s 2.3 million inhabitants from their homes since October 7.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', '‘', 'Al', '##arm', '##ing', '’', ':', 'Palestinians', 'a', '##cc', '##use', 'ICC', 'prosecutor', 'of', 'bias', 'after', 'Israel', 'visit', '|', 'During', 'his', 'three', '-', 'day', 'visit', ',', 'Israel', 'also', 'did', 'not', 'allow', 'Khan', 'to', 'enter', 'Gaza', ',', 'where', 'Israel', 'has', 'killed', 'more', 'than', '17', ',', '000', 'people', 'and', 'displaced', 'most', 'of', 'the', 'besieged', 'en', '##clave', '’', 's', '2', '.', '3', 'million', 'inhabitants', 'from', 'their', 'homes', 'since', 'October', '7', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138, 15425,  1370,  8619,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'A Strike For Palestine', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'A', 'Strike', 'For', 'Palestine', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5370,  1129,  1852,  1128,   117,  1106,  1103,   188, 12679,\n",
      "         27704,  1105,  1632,  1234,  1104,  8619,   119,   565, 28495, 28484,\n",
      "         28495, 28475, 26259,   583, 28495, 16070, 28494, 26259,   592, 28490,\n",
      "         28495, 28497,   578, 28490, 28476,   585, 28495, 28484, 28488, 16070,\n",
      "         17754,   565, 28495, 28486, 28475, 26259, 18191,   592, 28475, 28495,\n",
      "         28490, 28489, 16070, 26259,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Peace be upon you, to the steadfast and great people of Palestine.\\n السلام عليكم وعلى شعب فلسطين الصامد والعظيم.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Peace', 'be', 'upon', 'you', ',', 'to', 'the', 's', '##tead', '##fast', 'and', 'great', 'people', 'of', 'Palestine', '.', 'ا', '##ل', '##س', '##ل', '##ا', '##م', 'ع', '##ل', '##ي', '##ك', '##م', 'و', '##ع', '##ل', '##ى', 'ش', '##ع', '##ب', 'ف', '##ل', '##س', '##ط', '##ي', '##ن', 'ا', '##ل', '##ص', '##ا', '##م', '##د', 'و', '##ا', '##ل', '##ع', '##ظ', '##ي', '##م', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  7187,  2256,  1221,  1725,  1178,  1141,  1104,  1103,  1210,\n",
      "          5883,  2591,  1651,  1150,  2856, 14551, 16300,  1108,  6232,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Does anyone know why only one of the three NYU students who removed hostage posters was suspended?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Does', 'anyone', 'know', 'why', 'only', 'one', 'of', 'the', 'three', 'NY', '##U', 'students', 'who', 'removed', 'hostage', 'posters', 'was', 'suspended', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 11336, 14703,  1916,  1103,  3548,  1115,   112, 23755,   120,\n",
      "         15221,  1125,  1720,  1106,  2239,  1114,  1103, 12066,   112,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Refuting the claim that 'Palestinians/Arabs had nothing to deal with the Holocaust'\", 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Re', '##fu', '##ting', 'the', 'claim', 'that', \"'\", 'Palestinians', '/', 'Arabs', 'had', 'nothing', 'to', 'deal', 'with', 'the', 'Holocaust', \"'\", '[SEP]']}, {'title': {'input_ids': tensor([[  101,   139, 26859, 13981,  1279,  2757,  1106,  4582,  4890, 11251,\n",
      "          1106,  3103, 15872,  1748,  2935,  1107,  4340,  1192,  7221,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Biden bypasses Congress to sell tank shells to Israel amid further fighting in Khan Younis', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'B', '##iden', 'bypass', '##es', 'Congress', 'to', 'sell', 'tank', 'shells', 'to', 'Israel', 'amid', 'further', 'fighting', 'in', 'Khan', 'You', '##nis', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15109,  1179,  1534, 20651,  1115, 24574, 16867,  4256,   117,\n",
      "           107, 21820, 26502,  3052,   107,  9112,  1150,  1132,  2257,  1106,\n",
      "           107, 13803,   107,  1111,  8162,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': 'Gazan mother declares that Hamas steals aid, \"humiliates\" civilians who are forced to \"beg\" for bread.', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Gaza', '##n', 'mother', 'declares', 'that', 'Hamas', 'steals', 'aid', ',', '\"', 'hum', '##ilia', '##tes', '\"', 'civilians', 'who', 'are', 'forced', 'to', '\"', 'beg', '\"', 'for', 'bread', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24994,  1111,  5096,   118,  4878, 13918,   113,  1137,  2256,\n",
      "          1150,  3520,  1103,  6615,  1541,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Questions for Pro-Israeli folks (or anyone who knows the answers really)', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Questions', 'for', 'Pro', '-', 'Israeli', 'folks', '(', 'or', 'anyone', 'who', 'knows', 'the', 'answers', 'really', ')', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8329, 13409, 22158,  2144,   112,   189,  1928,  8329,  2217,\n",
      "          9084,  1596,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Antizionist doesn't mean Antisemitic.\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Anti', '##zio', '##nist', 'doesn', \"'\", 't', 'mean', 'Anti', '##se', '##mit', '##ic', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 3103, 1112, 1103, 4384, 1104, 6489,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israel as the Jews of Islam', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', 'as', 'the', 'Jews', 'of', 'Islam', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1706,  1155,  1103,   789, 24574,  1132,  4438,  7705,   120,\n",
      "          1110, 20439,  1233,  1132, 21146, 18323, 10396,  2145,   790, 13918,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'To all the “Hamas are freedom fighters/israel are propagandists” folks', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'To', 'all', 'the', '“', 'Hamas', 'are', 'freedom', 'fighters', '/', 'is', '##rae', '##l', 'are', 'prop', '##agan', '##dis', '##ts', '”', 'folks', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  5325, 24745,  1158, 19643,  1254,   119,  2185,  1110,\n",
      "          1567,  4547,   117,  2589, 10906,  2742,  1110,  5696,  1107,  3103,\n",
      "          1112,  1110,  2742,  1206,  1472, 13239,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}, 'orig_title': 'Israeli pinkwashing genocide again. Love is love apparently, except Gay marriage is illegal in Israel as is marriage between different religions', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'pink', '##wash', '##ing', 'genocide', 'again', '.', 'Love', 'is', 'love', 'apparently', ',', 'except', 'Gay', 'marriage', 'is', 'illegal', 'in', 'Israel', 'as', 'is', 'marriage', 'between', 'different', 'religions', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   164,   151, 19261,  2924,   166,  2586,   118,   154, 11192,\n",
      "          2312,  4292,  1116, 18751,  1158,   146,  2346,  2271,  1107,  4340,\n",
      "         27413,  1548,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': '[NSFW] Al-Qassam Brigades clashing IOF in Khan Yunis', 'label': 0, 'str_tokenized_sentence': ['[CLS]', '[', 'N', '##SF', '##W', ']', 'Al', '-', 'Q', '##ass', '##am', 'Brigade', '##s', 'clash', '##ing', 'I', '##O', '##F', 'in', 'Khan', 'Yun', '##is', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1192,  1169,   112,   189,  1294,  1142,  4170,  1146,   119,\n",
      "          2048,  8147,   117,  1122,   112,   188,  1110, 20439,  1233,   119,\n",
      "           118,  1110, 20439,  1233,  1867,  1489,   117,  2260,  1104,  1103,\n",
      "          1407,   117,  1288,  1703,  2103,  6209,  1107, 15109,  1127,  8988,\n",
      "         15598, 23928,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"You can't make this shit up. Oh yeah, it's israel. -israel says 14,500 of the 18,000 total reported deaths in Gaza were Palestinian Resistance Fighters.\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'You', 'can', \"'\", 't', 'make', 'this', 'shit', 'up', '.', 'Oh', 'yeah', ',', 'it', \"'\", 's', 'is', '##rae', '##l', '.', '-', 'is', '##rae', '##l', 'says', '14', ',', '500', 'of', 'the', '18', ',', '000', 'total', 'reported', 'deaths', 'in', 'Gaza', 'were', 'Palestinian', 'Resistance', 'Fighters', '.', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 2009, 1110, 3103, 4374, 7955, 1105, 1136, 9917,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Why is Israel supporting Azerbaijan and not Armenia?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'is', 'Israel', 'supporting', 'Azerbaijan', 'and', 'not', 'Armenia', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15109,  2360,  1280,  1106,  2270,  3769,  1146,  7333,  1107,\n",
      "         16890,  8057,  1324,  1118,  4878, 10095,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Gaza artist going to Ireland ends up trapped in Rafah by Israeli bombs', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Gaza', 'artist', 'going', 'to', 'Ireland', 'ends', 'up', 'trapped', 'in', 'Ra', '##fa', '##h', 'by', 'Israeli', 'bombs', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 19415,  1161, 17203,  1158,  5096,   118,  8619,  3438,  1113,\n",
      "          1130,  8419, 12139,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Meta suppressing Pro-Palestine content on Instagram', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Met', '##a', 'suppress', '##ing', 'Pro', '-', 'Palestine', 'content', 'on', 'In', '##sta', '##gram', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2038,  5811,  1104,  1293,  1103,   146,  2346,  2271, 16867,\n",
      "         11932,  1104,  1103,  2044,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Great coverage of how the IOF steals organs of the dead', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Great', 'coverage', 'of', 'how', 'the', 'I', '##O', '##F', 'steals', 'organs', 'of', 'the', 'dead', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3278,  2260, 27140,  2803,  4670,  1107, 15109,  1747,  2805,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Over 500 IDF soldiers wounded in Gaza ground operation', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Over', '500', 'IDF', 'soldiers', 'wounded', 'in', 'Gaza', 'ground', 'operation', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 19521, 13876,   118, 23274,  4648,  4107, 23755,  1164,  3103,\n",
      "          1105,  3519,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Corey Gil-Shuster asking Palestinians about Israel and peace', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Corey', 'Gil', '-', 'Shu', '##ster', 'asking', 'Palestinians', 'about', 'Israel', 'and', 'peace', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 8325, 7699, 7563, 1968, 1324,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Happy Hanukkah', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Happy', 'Han', '##uk', '##ka', '##h', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  1721,  1103,  1646,  1301,  1964,   787,   189,  1423,\n",
      "           118,  3541,  1193,  8192,   170,  7414,  6021,  1111,  1126,  5670,\n",
      "         14092,  7117,  1107, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}, 'orig_title': 'The moment the US gov’t single-handedly blocked a UN resolution for an immediate ceasefire in Gaza', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'The', 'moment', 'the', 'US', 'go', '##v', '’', 't', 'single', '-', 'handed', '##ly', 'blocked', 'a', 'UN', 'resolution', 'for', 'an', 'immediate', 'cease', '##fire', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  6602,   113, 17881,  1495,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israelism (2023)', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Israeli', '##sm', '(', '202', '##3', ')', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 14104,  3740,  2008,  5871,  7941,  1484,  7906,  1158,  1888,\n",
      "          1217,  8406,   120,  9645, 18581,  2064, 27370, 22441,  2137,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Shirtless hamas members surrendering video being fake/staged DEBUNKED', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Shi', '##rt', '##less', 'ha', '##mas', 'members', 'surrender', '##ing', 'video', 'being', 'fake', '/', 'staged', 'DE', '##B', '##UN', '##KE', '##D', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 16972, 11392,  3663,   106,  1456,  1237, 21232,   197,  7414,\n",
      "           197,  8619,  1105,  3103,  3725,  9216,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'TODAY! North American Doctors | UN | Palestine and Israel Update', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'TO', '##DA', '##Y', '!', 'North', 'American', 'Doctors', '|', 'UN', '|', 'Palestine', 'and', 'Israel', 'Up', '##date', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  4878,  6977,  2519,  2642,  1104,  4340,  1192,  7221,  1107,\n",
      "          1207,  4162,  1104,  2359, 15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Israeli tanks reach centre of Khan Younis in new storm of southern Gaza', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israeli', 'tanks', 'reach', 'centre', 'of', 'Khan', 'You', '##nis', 'in', 'new', 'storm', 'of', 'southern', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  8274,  9144,   143,  1204,   119, 17551,   148, 24486,  1233,\n",
      "           118,  8619,  3100,  5091,  5736,   113,  9018, 12118, 17248, 13841,\n",
      "          1953,  6301,   114,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Lowkey Ft. Mai Khalil - Palestine Will Never Die (Official Uncensored Music Video )', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Low', '##key', 'F', '##t', '.', 'Mai', 'K', '##hali', '##l', '-', 'Palestine', 'Will', 'Never', 'Die', '(', 'Official', 'Un', '##cens', '##ored', 'Music', 'Video', ')', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 27140,  6596,  1888,  1104, 24574, 11569,  4256,  1121, 15109,\n",
      "          2316,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'IDF releases video of Hamas stealing aid from Gazans', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'IDF', 'releases', 'video', 'of', 'Hamas', 'stealing', 'aid', 'from', 'Gaza', '##ns', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  2009,  1225,  1103,  7414,  1660,  3103,  1103,  2656,  1104,\n",
      "          1103,  1657,  2693,  1515,   170,  2964,  1416,   136,  1262,  1108,\n",
      "          1103, 23726,  3645,   112,   188, 10030,  1104,  1103,  2197,  5804,\n",
      "           112, 12394,   112,   117,  1114,  1103, 10010,  6457,  1104,  1781,\n",
      "          1166,  1155,  1104,  2268,  1810,  6207,  8619,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}, 'orig_title': \"Why did the UN give Israel the majority of the land despite having a smaller population? And was the Zionist leadership's acceptance of the plan merely 'tactical', with the ultimate aim of taking over all of Mandatory Palestine?\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Why', 'did', 'the', 'UN', 'give', 'Israel', 'the', 'majority', 'of', 'the', 'land', 'despite', 'having', 'a', 'smaller', 'population', '?', 'And', 'was', 'the', 'Zionist', 'leadership', \"'\", 's', 'acceptance', 'of', 'the', 'plan', 'merely', \"'\", 'tactical', \"'\", ',', 'with', 'the', 'ultimate', 'aim', 'of', 'taking', 'over', 'all', 'of', 'Man', '##da', '##tory', 'Palestine', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   585, 16070,   589, 28496, 28493, 28492,   566, 28488, 28496,\n",
      "         28495, 16070,   119,   119,   581, 28492, 28495,   594, 28480, 28477,\n",
      "         28487, 17754,   565, 28481, 28477, 15389,   565, 28495, 28486, 28491,\n",
      "         16070, 19775, 23525,   568, 28480, 28477,   575, 28494, 28475, 26259,\n",
      "           589, 17754, 28483, 28495, 15389, 26259,   565, 28495, 28482, 16070,\n",
      "           573, 26259, 19775, 28477, 15389,   565, 28495, 28491, 28475, 19775,\n",
      "         28475, 28477,   565, 28495, 28475, 19775, 28484, 19775, 28475, 28474,\n",
      "         16070, 28495, 16070, 23525,   119,   119,   592, 16070, 28481, 19775,\n",
      "         28479, 15389, 28475,   589, 17754,   566, 16070, 17754,   565, 28495,\n",
      "         19775, 28494, 28475, 26259,   592, 28475, 28495, 18191, 26259, 28475,\n",
      "         19775,   119,   119,   108,   584, 28483, 23525,   108,   565, 28484,\n",
      "         19775, 28475, 28474, 16070, 28495,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'في موقف بطولي.. طفل يحتضن اخته الصغيرة تحت ركام منزلهم الذي دمرته الغارات الارسرائيلية.. ويخرجها من بين الركام والدمار..\\n\\n#غزة #اسرائيل ', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'ف', '##ي', 'م', '##و', '##ق', '##ف', 'ب', '##ط', '##و', '##ل', '##ي', '.', '.', 'ط', '##ف', '##ل', 'ي', '##ح', '##ت', '##ض', '##ن', 'ا', '##خ', '##ت', '##ه', 'ا', '##ل', '##ص', '##غ', '##ي', '##ر', '##ة', 'ت', '##ح', '##ت', 'ر', '##ك', '##ا', '##م', 'م', '##ن', '##ز', '##ل', '##ه', '##م', 'ا', '##ل', '##ذ', '##ي', 'د', '##م', '##ر', '##ت', '##ه', 'ا', '##ل', '##غ', '##ا', '##ر', '##ا', '##ت', 'ا', '##ل', '##ا', '##ر', '##س', '##ر', '##ا', '##ئ', '##ي', '##ل', '##ي', '##ة', '.', '.', 'و', '##ي', '##خ', '##ر', '##ج', '##ه', '##ا', 'م', '##ن', 'ب', '##ي', '##ن', 'ا', '##ل', '##ر', '##ك', '##ا', '##م', 'و', '##ا', '##ل', '##د', '##م', '##ا', '##ر', '.', '.', '#', 'غ', '##ز', '##ة', '#', 'ا', '##س', '##ر', '##ا', '##ئ', '##ي', '##ل', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 6405, 6209, 1107, 8755,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Child deaths in wars', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Child', 'deaths', 'in', 'wars', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 23755,  1138,  2379,  2266,  1315,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinians have natural rights too', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Palestinians', 'have', 'natural', 'rights', 'too', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 23755,  1840,  1111,  4265,  4585,  1113,  6356,  1106,  4555,\n",
      "          5670, 14092,  7117,   197,  3103,   118,  8619,  4139,  3128,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Palestinians call for global strike on Monday to demand immediate ceasefire | Israel-Palestine conflict News', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Palestinians', 'call', 'for', 'global', 'strike', 'on', 'Monday', 'to', 'demand', 'immediate', 'cease', '##fire', '|', 'Israel', '-', 'Palestine', 'conflict', 'News', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 19536,  4179,  2304,  1111,  1251,  8828,  5096,   118,  8988,\n",
      "           789,  4299,  8619,   790,  1133,  2848,   118,  9640,  5191,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Serious question for any moderate Pro-Palestinian “Free Palestine” but anti-terrorist folk', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Ser', '##ious', 'question', 'for', 'any', 'moderate', 'Pro', '-', 'Palestinian', '“', 'Free', 'Palestine', '”', 'but', 'anti', '-', 'terrorist', 'folk', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15756, 11405,  7189,  1293,  3103,  1841, 24428,  1107, 14477,\n",
      "         13834, 27578,  4108,   787, 14044,  1182,  1113,  1357,   128,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Fresh testimony reveals how Israel killed captives in Kibbutz Be’eri on October 7', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Fresh', 'testimony', 'reveals', 'how', 'Israel', 'killed', 'captives', 'in', 'Ki', '##bb', '##utz', 'Be', '’', 'er', '##i', 'on', 'October', '7', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15531,  2591,  3190,   131, 20820, 18266, 23222,   111, 24489,\n",
      "         10860,  1851,   118, 14321,  6140, 26385,  7268,  7277,  2386,  2733,\n",
      "         14923,  5031,  3103,   787,   188,  7382,  1706, 19590, 15109, 12872,\n",
      "          1116,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'CRUX: Netanyahu & Putin Hold 50-Minute Phone Call Amid Russia Slamming Israel’s Plan To Flood Gaza Tunnels', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'CR', '##U', '##X', ':', 'Net', '##any', '##ahu', '&', 'Putin', 'Hold', '50', '-', 'Min', '##ute', 'Phone', 'Call', 'Am', '##id', 'Russia', 'Slam', '##ming', 'Israel', '’', 's', 'Plan', 'To', 'Flood', 'Gaza', 'Tunnel', '##s', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1239,  1104,  1756, 17297,  7539,  1651, 10137, 19643,  1107,\n",
      "         15109,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'University of California Irvine targets students opposing genocide in Gaza', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'University', 'of', 'California', 'Irvine', 'targets', 'students', 'opposing', 'genocide', 'in', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1327, 1202, 8619, 6638, 1341, 1104, 3398,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What do Palestine supporters think of Iran?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'do', 'Palestine', 'supporters', 'think', 'of', 'Iran', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  5096, 27140,  5756,  1208,  8525,  3516,  1111, 19643,  1105,\n",
      "          5237,  4044,  4253,  1104,  1155, 23755,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Pro IDF accounts now actively calling for genocide and ethnic cleansing of all Palestinians', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Pro', 'IDF', 'accounts', 'now', 'actively', 'calling', 'for', 'genocide', 'and', 'ethnic', 'clean', '##sing', 'of', 'all', 'Palestinians', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   100,   535, 28456, 28458, 28460, 28461, 28454,   117,   546,\n",
      "         28467, 28462, 28446, 28466,   557, 28466,   539, 28468, 28465, 28467,\n",
      "         28450, 28466, 28468,   540, 28449, 28446, 28454, 28460, 28450, 28454,\n",
      "           536, 28458, 28458, 28467, 28456, 28468,   542, 28458, 28445, 28461,\n",
      "           131,   551, 28454, 28460, 28450, 28445, 28466,   548, 28460, 28449,\n",
      "         28454, 28447,   542, 28446, 28450, 28466, 28468,   548, 28467, 28450,\n",
      "         28447, 28462, 28454, 28457,   117,   542, 28454, 28454, 28446, 28454,\n",
      "         28457,   546, 28449, 28454, 28463, 28453, 28466,   548, 28458, 28460,\n",
      "         28450,   117,   539, 28452, 28451, 28454, 28466,   535, 28468,   556,\n",
      "         28464, 28450, 28462, 28468,   552, 28451, 28449,  2363,   557, 28460,\n",
      "         28449,   546, 28445, 28452, 28450, 28466,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'יוסף אלמנסי, לשעבר שר התקשורת והבינוי בממשלת חמאס: סינואר מנהיג חבורת משוגעים, חייבים להיפטר ממנו, החזיר את רצועת עזה 200 שנה לאחור', 'label': -1, 'str_tokenized_sentence': ['[CLS]', '[UNK]', 'א', '##ל', '##מ', '##נ', '##ס', '##י', ',', 'ל', '##ש', '##ע', '##ב', '##ר', 'ש', '##ר', 'ה', '##ת', '##ק', '##ש', '##ו', '##ר', '##ת', 'ו', '##ה', '##ב', '##י', '##נ', '##ו', '##י', 'ב', '##מ', '##מ', '##ש', '##ל', '##ת', 'ח', '##מ', '##א', '##ס', ':', 'ס', '##י', '##נ', '##ו', '##א', '##ר', 'מ', '##נ', '##ה', '##י', '##ג', 'ח', '##ב', '##ו', '##ר', '##ת', 'מ', '##ש', '##ו', '##ג', '##ע', '##י', '##ם', ',', 'ח', '##י', '##י', '##ב', '##י', '##ם', 'ל', '##ה', '##י', '##פ', '##ט', '##ר', 'מ', '##מ', '##נ', '##ו', ',', 'ה', '##ח', '##ז', '##י', '##ר', 'א', '##ת', 'ר', '##צ', '##ו', '##ע', '##ת', 'ע', '##ז', '##ה', '200', 'ש', '##נ', '##ה', 'ל', '##א', '##ח', '##ו', '##ר', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   118,  8619,  1594,   131, 26159,  1104,  4878,  2803,\n",
      "          4670,  1105,   112, 10302,   112,  1107, 15109,  2935,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Israel-Palestine war: Thousands of Israeli soldiers wounded and 'disabled' in Gaza fighting\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', '-', 'Palestine', 'war', ':', 'Thousands', 'of', 'Israeli', 'soldiers', 'wounded', 'and', \"'\", 'disabled', \"'\", 'in', 'Gaza', 'fighting', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 26744,  1348,   148, 11941,  1181,   131,  1422, 12015,  1622,\n",
      "          1291,  1635, 10768,  1106, 15914,  6660, 15109, 10283,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Belal Khaled: My Journey From World Cup Joy to Capturing Gaza grief.', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Bel', '##al', 'K', '##hale', '##d', ':', 'My', 'Journey', 'From', 'World', 'Cup', 'Joy', 'to', 'Capt', '##uring', 'Gaza', 'grief', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1301, 14703,  3276,  3263,  1111,   183,  9379,  2377,  6232,\n",
      "          1111,  4554,  2050, 17054, 18945,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'gofundme for nyu student suspended for palestinian activism', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'go', '##fu', '##nd', '##me', 'for', 'n', '##yu', 'student', 'suspended', 'for', 'pale', '##st', '##inian', 'activism', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   112,   146,  1821,  1128,   112,   131, 17520,   159, 24874,\n",
      "          1233, 25796,  8988, 22381,  1987,   119, 11336,  8057,  2980,  2586,\n",
      "          8836,  1200,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"'I am you': NYC Vigil Honors Palestinian Poet Dr. Refaat Alareer\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', \"'\", 'I', 'am', 'you', \"'\", ':', 'NYC', 'V', '##igi', '##l', 'Honors', 'Palestinian', 'Poet', 'Dr', '.', 'Re', '##fa', '##at', 'Al', '##are', '##er', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1188,  1594,  1110,  3665,  1103,  6088,  1104, 24574,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'This war is entirely the fault of Hamas', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'This', 'war', 'is', 'entirely', 'the', 'fault', 'of', 'Hamas', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  3103,   118, 15109,  1594,   131,  8636,  1104, 15109,   112,\n",
      "           188,  1416,  1110, 20285,   117, 21310,  7414,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Israel-Gaza war: Half of Gaza's population is starving, warns UN\", 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Israel', '-', 'Gaza', 'war', ':', 'Half', 'of', 'Gaza', \"'\", 's', 'population', 'is', 'starving', ',', 'warns', 'UN', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 20452, 10747,  1158,  4737,  9198, 13335,  3269,   118,  3036,\n",
      "         18613, 21263,  1161,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Scrolling Through Genocide - Steve Salaita', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Sc', '##roll', '##ing', 'Through', 'Gen', '##oc', '##ide', '-', 'Steve', 'Sal', '##ait', '##a', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  1110,  1940,  1116,  8702,  3384, 18437,  1115,  1657,\n",
      "          1107, 15109,   117,  3103,  1163,  2052,   118,  2457,  1396, 12231,\n",
      "           136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas is misfiring rockets that land in Gaza, Israel said today - independent verification?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'is', 'mi', '##s', '##fi', '##ring', 'rockets', 'that', 'land', 'in', 'Gaza', ',', 'Israel', 'said', 'today', '-', 'independent', 've', '##rification', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 10656,   112,   188,  1103,  2295,   119, 15425,  1111, 15109,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': \"Tomorrow's the Day. Strike for Gaza\", 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Tomorrow', \"'\", 's', 'the', 'Day', '.', 'Strike', 'for', 'Gaza', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 15628,  1114,  2053,   117,  2046,  2044,  1118,  3103,   131,\n",
      "           138, 16705,  1394,  2298,   787,   188,  1509,  4899,   197,  3103,\n",
      "           118,  8619,  4139,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Playing with friends, shot dead by Israel: A Jenin boy’s final moments | Israel-Palestine conflict', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Playing', 'with', 'friends', ',', 'shot', 'dead', 'by', 'Israel', ':', 'A', 'Jen', '##in', 'boy', '’', 's', 'final', 'moments', '|', 'Israel', '-', 'Palestine', 'conflict', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   163,  4626,  2322, 20304,  1171,  9936,  1170, 14634,  1106,\n",
      "         15109,  5915,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Zara campaign sparks backlash after resemblance to Gaza destruction', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'Z', '##ara', 'campaign', 'sparks', 'back', '##lash', 'after', 'resemblance', 'to', 'Gaza', 'destruction', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  3123,   126, 26063,  3442,  1106,  9474,  4699,   118,\n",
      "          4878,  4139,   119, 16896,  1105, 13036,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The easy 5 yo method to solve Arab-Israeli conflict. Simple and applicable.', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'The', 'easy', '5', 'yo', 'method', 'to', 'solve', 'Arab', '-', 'Israeli', 'conflict', '.', 'Simple', 'and', 'applicable', '.', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  3478,  1817, 12778,  1111,  3655,  7680,   118,  2592,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas leaders leave Qatar for unknown destination - report', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'leaders', 'leave', 'Qatar', 'for', 'unknown', 'destination', '-', 'report', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   138,  8661, 17442,  1162,  1120,  7414,  1112,  1615,  2970,\n",
      "          3667,  1106,  1171, 15109, 14092,  7117,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Applause at UN as General Assembly votes to back Gaza ceasefire', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'A', '##pp', '##laus', '##e', 'at', 'UN', 'as', 'General', 'Assembly', 'votes', 'to', 'back', 'Gaza', 'cease', '##fire', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   107,  7102,  1185,  6223,   119,  4081,  2704,  1105,  1278,\n",
      "         20812,   117,  1451,  1590,  1105,  2027,  6636,  1107, 15109,  8807,\n",
      "          1103,  3602, 12668,  1104,  1103,  1646,  1433,   119,  1109,  2061,\n",
      "          1585,  4381,  1103,  5237,  4044,  4253,  2446,  1149,  1118,  3103,\n",
      "          1107,  8619,   119,   107,   118,  5124,  2001,  7926,  3101,   117,\n",
      "          5468, 23095,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}, 'orig_title': '\"Make no mistake. Every hospital and school bombed, every woman and child murdered in Gaza bears the fingerprints of the US government. The White House funds the ethnic cleansing carried out by Israel in Palestine.\" - Carlos Latuff, Brazilian cartoonist', 'label': 1, 'str_tokenized_sentence': ['[CLS]', '\"', 'Make', 'no', 'mistake', '.', 'Every', 'hospital', 'and', 'school', 'bombed', ',', 'every', 'woman', 'and', 'child', 'murdered', 'in', 'Gaza', 'bears', 'the', 'finger', '##prints', 'of', 'the', 'US', 'government', '.', 'The', 'White', 'House', 'funds', 'the', 'ethnic', 'clean', '##sing', 'carried', 'out', 'by', 'Israel', 'in', 'Palestine', '.', '\"', '-', 'Carlos', 'La', '##tu', '##ff', ',', 'Brazilian', 'cartoonist', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1327,  1110,  6489,  4184,  5114, 10242,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'What is Islamophobia?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'What', 'is', 'Islam', '##op', '##ho', '##bia', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1135,   787,   188,  1136, 19196,  1106,  1267,  4878,  1482,\n",
      "          8294,  1103, 15109, 19643,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'It’s not shocking to see Israeli children celebrate the Gaza genocide', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'It', '’', 's', 'not', 'shocking', 'to', 'see', 'Israeli', 'children', 'celebrate', 'the', 'Gaza', 'genocide', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  6523, 17328,  1138,  1562,  2037,  1164,  1103, 15167,  1104,\n",
      "          2212,   170,  1550,  4384,  1121,  4699,  4508,  1105,  3398,   136,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Barely have seen talk about the displacement of nearly a million Jews from Arab lands and Iran?', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Bar', '##ely', 'have', 'seen', 'talk', 'about', 'the', 'displacement', 'of', 'nearly', 'a', 'million', 'Jews', 'from', 'Arab', 'lands', 'and', 'Iran', '?', '[SEP]']}, {'title': {'input_ids': tensor([[  101, 24574,  1215, 12844,  9556,  1106,  2311, 24883,  1348, 16075,\n",
      "          2830,  1113, 14125,   119,   128,   117, 27140, 17357,  1163,  1106,\n",
      "          1437,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'Hamas used toxic substance to kill Nahal Oz troops on Oct. 7, IDF probe said to show', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'Hamas', 'used', 'toxic', 'substance', 'to', 'kill', 'Nah', '##al', 'Oz', 'troops', 'on', 'Oct', '.', '7', ',', 'IDF', 'probe', 'said', 'to', 'show', '[SEP]']}, {'title': {'input_ids': tensor([[  101,   146,  1138,  1309,  1464,  1167,  9221,  2913,  1107,  1139,\n",
      "          2848, 24226, 14200,  1204,  8810,  1190,  1702,  1120,  1184,  1103,\n",
      "         27140,  1209,  1129,  1833,  1106,  1103, 23755,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}, 'orig_title': 'I have never felt more validated in my antinatalist beliefs than looking at what the IDF will be doing to the Palestinians', 'label': 1, 'str_tokenized_sentence': ['[CLS]', 'I', 'have', 'never', 'felt', 'more', 'valid', '##ated', 'in', 'my', 'anti', '##nat', '##alis', '##t', 'beliefs', 'than', 'looking', 'at', 'what', 'the', 'IDF', 'will', 'be', 'doing', 'to', 'the', 'Palestinians', '[SEP]']}, {'title': {'input_ids': tensor([[  101,  1109,  3248,  2008,  1218,  1104,  9712, 12233,  1579,  3093,\n",
      "          1106,  1576,  3712,  1165,  1122,  2502,  1106,  4384,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'The bottomless well of empathy always seems to run dry when it comes to Jews', 'label': -1, 'str_tokenized_sentence': ['[CLS]', 'The', 'bottom', '##less', 'well', 'of', 'em', '##pathy', 'always', 'seems', 'to', 'run', 'dry', 'when', 'it', 'comes', 'to', 'Jews', '[SEP]']}, {'title': {'input_ids': tensor([[ 101, 1335, 1184, 1553, 1431, 1103, 2805, 1831,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'orig_title': 'At what point should the operation stop?', 'label': 0, 'str_tokenized_sentence': ['[CLS]', 'At', 'what', 'point', 'should', 'the', 'operation', 'stop', '?', '[SEP]']}]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_rte(data, tokenizer):\n",
    "  tokenized = []\n",
    "  for inst in data:\n",
    "    n_inst = {\n",
    "      'title': tokenizer(inst['title'], return_tensors='pt'),\n",
    "      'orig_title': inst['title'],\n",
    "      'label': inst['label'],\n",
    "    }\n",
    "    n_inst['str_tokenized_sentence'] = tokenizer.convert_ids_to_tokens(n_inst['title']['input_ids'][0])\n",
    "    tokenized.append(n_inst)\n",
    "\n",
    "  return tokenized\n",
    "\n",
    "tokenized_training_data = tokenize_rte(training_data, tokenizer)\n",
    "print(tokenized_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_inst, data_labels, weight_adjuster, loss_fn,\n",
    "          batch_size=100, num_epochs=5, epoch_callback=None):\n",
    "  for epoch in range(num_epochs):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    batch_start = 0\n",
    "    batch_end = min(batch_size, len(data_inst))\n",
    "    num_batches = int(numpy.ceil(len(data_inst) / batch_size))\n",
    "    for batch_idx, batch in enumerate(range(num_batches)):\n",
    "      batch = data_inst[batch_start:batch_end]\n",
    "      weight_adjuster.zero_grad()\n",
    "      logits = model(batch)\n",
    "\n",
    "      labels = data_labels[batch_start:batch_end]\n",
    "\n",
    "      loss = loss_fn(input=logits, target=labels)\n",
    "      print(\"Epoch %d, Batch %d: %d --> %d, Batch loss %f\" % (epoch, batch_idx, batch_start, batch_end, loss.item()))\n",
    "      loss.backward()\n",
    "      weight_adjuster.step()\n",
    "\n",
    "      batch_start = batch_end\n",
    "      batch_end = batch_end+batch_size\n",
    "      batch_end = min(batch_end, len(data_inst))\n",
    "\n",
    "    if epoch_callback is not None:\n",
    "      epoch_callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "embedder = BinaryWordOverlapBatched(vocab_size)\n",
    "\n",
    "model = MyClassifier(embedder)\n",
    "\n",
    "batches = tokenized_training_data[0:2]\n",
    "logits = model(batches)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "weight_adjuster = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target -1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_inst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_training_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_true_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweight_adjuster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_adjuster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_inst, data_labels, weight_adjuster, loss_fn, batch_size, num_epochs, epoch_callback)\u001b[0m\n\u001b[0;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[0;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m data_labels[batch_start:batch_end]\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m --> \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Batch loss \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, batch_idx, batch_start, batch_end, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target -1 is out of bounds."
     ]
    }
   ],
   "source": [
    "train(model=model,\n",
    "      data_inst=tokenized_training_data,\n",
    "      data_labels=torch.tensor(training_true_labels),\n",
    "      weight_adjuster=weight_adjuster,\n",
    "      loss_fn=loss_fn,\n",
    "      batch_size = 10,\n",
    "      num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, true_labels, batch_size):\n",
    "  dev_data = tokenize_rte(rte_dev, tokenizer)\n",
    "  model.eval()\n",
    "  model_eval = model(dev_data[0:30])\n",
    "\n",
    "  predictions = []\n",
    "  for prediction in model_eval:\n",
    "    predictions.append(0 if prediction[0] > prediction[1] else 1)\n",
    "\n",
    "  true_labels = true_labels[:batch_size]\n",
    "\n",
    "  accuary = accuracy_score(true_labels, predictions)\n",
    "  print(f\"Accuracy = {accuary}\")\n",
    "\n",
    "  macro_precision = precision_score(true_labels, predictions, average='macro')\n",
    "  macro_recall = recall_score(true_labels, predictions, average='macro')\n",
    "  print(f\"Macro Precision = {macro_precision}\")\n",
    "  print(f\"Macro Recall = {macro_recall}\")\n",
    "\n",
    "  micro_precision = precision_score(true_labels, predictions, average='micro')\n",
    "  micro_recall = recall_score(true_labels, predictions, average='micro')\n",
    "  print(f\"Micro Precision = {micro_precision}\")\n",
    "  print(f\"Micro Recall = {micro_recall}\")\n",
    "\n",
    "evaluate(model, true_labels_dev, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
